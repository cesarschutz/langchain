# üìö Anota√ß√µes de Estudo - LangChain

Este arquivo cont√©m anota√ß√µes e explica√ß√µes sobre cada script do projeto para facilitar o estudo e consulta.

## üìã √çNDICE R√ÅPIDO

### üåç PASTA 1: FUNDAMENTOS
- **Script 1**: ChatOpenAI - Como usar modelos OpenAI (GPT) com par√¢metros
- **Script 2**: init_chat_model - Como usar modelos de diferentes provedores (Google, OpenAI)
- **Script 3**: PromptTemplate - Como criar prompts reutiliz√°veis com vari√°veis
- **Script 4**: ChatPromptTemplate - Como criar conversas estruturadas (system, user, assistant)

### üîß PASTA 2: CHAINS E PROCESSAMENTO
- **Script 1**: LCEL e pipe operator (`|`) - Como conectar componentes em sequ√™ncia
- **Script 2**: @chain decorator - Como criar fun√ß√µes customizadas que funcionam em chains
- **Script 3**: RunnableLambda - Como integrar fun√ß√µes simples em chains
- **Script 4**: Pipeline complexo - Como criar processamento com m√∫ltiplas etapas
- **Script 5**: Sumariza√ß√£o "stuff" - Como resumir textos curtos em uma √∫nica opera√ß√£o
- **Script 6**: Sumariza√ß√£o "map_reduce" - Como resumir textos muito longos em duas etapas
- **Script 7**: Pipeline manual map-reduce - Como controlar completamente o processo de sumariza√ß√£o

### ü§ñ PASTA 3: AGENTES E TOOLS
- **Script 1**: ReAct agent e tools - Como criar agentes que pensam e usam ferramentas
- **Script 2**: Prompt Hub - Como usar prompts otimizados da comunidade

### üß† PASTA 4: GERENCIAMENTO DE MEM√ìRIA
- **Script 1**: Armazenamento de hist√≥rico - Como fazer o modelo lembrar conversas anteriores
- **Script 2**: Sliding window - Como controlar o tamanho da mem√≥ria para economizar tokens

### üìö PASTA 5: LOADERS E BANCO DE DADOS VETORIAIS
- **Script 1**: WebBaseLoader - Como carregar conte√∫do de p√°ginas web
- **Script 2**: PyPDFLoader - Como extrair texto de arquivos PDF
- **Script 3**: Ingest√£o PGVector - Como armazenar documentos em banco vetorial PostgreSQL
- **Script 4**: Busca vetorial - Como encontrar documentos similares usando busca sem√¢ntica

## üéØ CONCEITOS FUNDAMENTAIS

### üîë Componentes Principais
- **Models**: ChatOpenAI, init_chat_model
- **Prompts**: PromptTemplate, ChatPromptTemplate
- **Chains**: LCEL, @chain, RunnableLambda
- **Agents**: ReAct, tools, Prompt Hub
- **Memory**: InMemoryChatMessageHistory, RunnableWithMessageHistory
- **Loaders**: WebBaseLoader, PyPDFLoader
- **Vector Stores**: PGVector, OpenAIEmbeddings

### üîÑ Fluxos Comuns
1. **Processamento b√°sico**: Prompt ‚Üí Model ‚Üí Response
2. **Chain complexa**: Input ‚Üí Process ‚Üí Output
3. **Agente**: Question ‚Üí Thought ‚Üí Action ‚Üí Observation
4. **Mem√≥ria**: Input ‚Üí History ‚Üí Context ‚Üí Response
5. **Vector DB**: Document ‚Üí Embedding ‚Üí Store ‚Üí Search

---

# üåç PASTA 1: FUNDAMENTOS

---

## üåç Script 1: 1-fundamentos/1-hello-world.py

### Explica√ß√£o do ChatOpenAI:
```python
ChatOpenAI(model="gpt-5-nano", temperature=0.5)
```

**Par√¢metros:**
- **`model="gpt-5-nano"`**: Especifica qual modelo da OpenAI usar
  - `gpt-5-nano`: Modelo mais r√°pido e econ√¥mico
  - Outras op√ß√µes: `gpt-4`, `gpt-3.5-turbo`, `gpt-4-turbo`
- **`temperature=0.5`**: Controla a criatividade das respostas
  - `0.0`: Respostas muito determin√≠sticas e consistentes
  - `0.5`: Equil√≠brio entre criatividade e consist√™ncia
  - `1.0`: Respostas muito criativas e variadas

**Quando usar:**
- Para projetos espec√≠ficos da OpenAI
- Quando voc√™ precisa de controle fino sobre os par√¢metros
- Para integra√ß√µes diretas com modelos OpenAI

---

## üîß Script 2: 1-fundamentos/2-init-chat-model.py

### Explica√ß√£o do init_chat_model:
```python
gemini = init_chat_model(model="gemini-2.5-flash", model_provider="google_genai")
```

**Par√¢metros:**
- **`model="gemini-2.5-flash"`**: Especifica o modelo espec√≠fico do provedor
  - `gemini-2.5-flash`: Modelo r√°pido do Google
  - Outras op√ß√µes: `gemini-2.0-flash`, `gemini-1.5-pro`
- **`model_provider="google_genai"`**: Indica qual provedor usar
  - `google_genai`: Para modelos Google Gemini
  - `openai`: Para modelos OpenAI
  - `anthropic`: Para modelos Anthropic Claude

**Vantagens:**
- Flexibilidade para trocar entre provedores
- C√≥digo mais gen√©rico e reutiliz√°vel
- Suporte a m√∫ltiplos provedores de IA

**Quando usar:**
- Para projetos que podem usar diferentes provedores
- Quando voc√™ quer flexibilidade para trocar modelos
- Para c√≥digo mais port√°vel entre diferentes servi√ßos

---

## üìù Script 3: 1-fundamentos/3-prompt-template.py

### Explica√ß√£o do PromptTemplate:
```python
template = PromptTemplate(
    input_variables=["name"],
    template="Hi, I'm {name}! Tell me a joke with my name!"
)
```

**Par√¢metros:**
- **`input_variables=["name"]`**: Lista das vari√°veis que o template aceita
  - Define quais placeholders podem ser usados no template
  - Pode ter m√∫ltiplas vari√°veis: `["name", "age", "city"]`
- **`template="..."`**: O texto do prompt com placeholders
  - Use `{variavel}` para criar placeholders
  - O texto pode ser qualquer prompt que voc√™ queira reutilizar

### Explica√ß√£o do m√©todo format():
```python
text = template.format(name="Wesley")
```

**Funcionalidade:**
- **Substitui as vari√°veis** por valores reais
- **`name="Wesley"`**: Substitui `{name}` por "Wesley" no template
- **Retorna o prompt final** pronto para ser usado

### Vantagens dos Templates:
- **Reutiliza√ß√£o**: Crie um template e use com diferentes valores
- **Consist√™ncia**: Mant√©m a estrutura do prompt sempre igual
- **Manutenibilidade**: Mude o template uma vez, afeta todos os usos
- **Legibilidade**: C√≥digo mais limpo e organizado

### Exemplo pr√°tico:
```python
# Template criado uma vez
template = PromptTemplate(
    input_variables=["name", "age"],
    template="Ol√° {name}, voc√™ tem {age} anos!"
)

# Usado m√∫ltiplas vezes
text1 = template.format(name="Jo√£o", age="25")
text2 = template.format(name="Maria", age="30")
# Resultado: "Ol√° Jo√£o, voc√™ tem 25 anos!" e "Ol√° Maria, voc√™ tem 30 anos!"
```

---

## üí¨ Script 4: 1-fundamentos/4-chat-prompt-template.py

### Explica√ß√£o do ChatPromptTemplate:
```python
system = ("system", "you are an assistant that answers questions in a {style} style")
user = ("user", "{question}")

chat_prompt = ChatPromptTemplate([system, user])
```

**Diferen√ßa do PromptTemplate simples:**
- **PromptTemplate**: Cria um texto √∫nico com vari√°veis
- **ChatPromptTemplate**: Cria m√∫ltiplas mensagens estruturadas (system, user, assistant)

**Estrutura das mensagens:**
- **`("system", "...")`**: Define o papel/comportamento da IA
  - Controla como a IA deve se comportar
  - Pode definir estilo, tom, especializa√ß√£o
- **`("user", "...")`**: Define a entrada/pergunta do usu√°rio
  - O que o usu√°rio est√° perguntando
  - Pode conter vari√°veis para personaliza√ß√£o

### Explica√ß√£o do format_messages():
```python
messages = chat_prompt.format_messages(style="funny", question="Who is Alan Turing?")
```

**Funcionalidade:**
- **Substitui vari√°veis** em todas as mensagens do template
- **Retorna lista de mensagens** estruturadas
- **Preserva a estrutura** system/user/assistant

### Vantagens do ChatPromptTemplate:
- **Estrutura de conversa**: Mant√©m o contexto de chat
- **Controle de comportamento**: System message define o papel da IA
- **Flexibilidade**: Pode ter m√∫ltiplas mensagens (system, user, assistant)
- **Compatibilidade**: Funciona melhor com modelos de chat

### Exemplo de sa√≠da:
```python
# Ap√≥s format_messages():
# system: you are an assistant that answers questions in a funny style
# user: Who is Alan Turing?

# Ap√≥s invoke():
# assistant: [Resposta engra√ßada sobre Alan Turing]
```

### Compara√ß√£o com PromptTemplate:
```python
# PromptTemplate (simples):
template = PromptTemplate(
    input_variables=["name"],
    template="Hi, I'm {name}!"
)

# ChatPromptTemplate (estruturado):
chat_prompt = ChatPromptTemplate([
    ("system", "You are a helpful assistant"),
    ("user", "Tell me about {topic}")
])
```

---

# üîó PASTA 2: CHAINS E PROCESSAMENTO

---

## üîó Script 1: 2-chains-e-processamento/1-iniciando-com-chains.py

### Explica√ß√£o do operador pipe (|):
```python
chain = question_template | model
```

**O que √© o operador |:**
- **LCEL (LangChain Expression Language)**: Sintaxe para conectar componentes
- **Operador pipe (|)**: Conecta componentes em sequ√™ncia
- **Pipeline**: Cria um fluxo de processamento: template ‚Üí modelo

**Como funciona:**
1. **question_template**: Formata o prompt com as vari√°veis
2. **|**: Conecta o template formatado ao modelo
3. **model**: Recebe o prompt formatado e gera a resposta

### Explica√ß√£o do invoke com dicion√°rio:
```python
result = chain.invoke({"name": "Wesley"})
```

**Diferen√ßa do invoke simples:**
- **Modelo simples**: `model.invoke("texto direto")`
- **Chain**: `chain.invoke({"variavel": "valor"})`

**Vantagens das Chains:**
- **Modularidade**: Cada componente tem uma fun√ß√£o espec√≠fica
- **Reutiliza√ß√£o**: Pode trocar componentes facilmente
- **Legibilidade**: C√≥digo mais limpo e organizado
- **Flexibilidade**: Pode adicionar mais componentes na sequ√™ncia

### Exemplo de fluxo:
```python
# 1. Template formata: "Ola, eu sou o Wesley! conteme uma piada!"
# 2. Modelo recebe o texto formatado
# 3. Modelo gera a resposta
# 4. Resultado final √© retornado
```

---

## üîß Script 2: 2-chains-e-processamento/2-chains-com-decorators.py

### Explica√ß√£o do decorador @chain:
```python
@chain
def square(input_dict:dict) -> dict:
    x = input_dict["x"]
    return {"square_result": x * x}
```

**O que √© o decorador @chain:**
- **Transforma fun√ß√µes Python** em componentes compat√≠veis com LCEL
- **Permite criar l√≥gica customizada** entre componentes do LangChain
- **Mant√©m a compatibilidade** com o operador pipe (|)

**Estrutura da fun√ß√£o:**
- **`input_dict:dict`**: Recebe um dicion√°rio como entrada
- **`-> dict`**: Retorna um dicion√°rio como sa√≠da
- **Deve seguir o padr√£o**: entrada e sa√≠da como dicion√°rios

### Explica√ß√£o da chain com 3 componentes:
```python
chain2 = square | question_template2 | model
```

**Fluxo de execu√ß√£o:**
1. **square**: Recebe `{"x": 10}` ‚Üí retorna `{"square_result": 100}`
2. **question_template2**: Recebe `{"square_result": 100}` ‚Üí formata prompt
3. **model**: Recebe o prompt formatado ‚Üí gera resposta

### Vantagens das fun√ß√µes personalizadas:
- **L√≥gica customizada**: Pode adicionar qualquer processamento
- **Reutiliza√ß√£o**: Fun√ß√µes podem ser usadas em diferentes chains
- **Flexibilidade**: Pode conectar l√≥gica Python pura com LangChain
- **Testabilidade**: Fun√ß√µes podem ser testadas independentemente

### Exemplo de uso:
```python
# Fun√ß√£o personalizada
@chain
def process_data(input_dict: dict) -> dict:
    data = input_dict["data"]
    processed = data.upper()  # Qualquer l√≥gica Python
    return {"processed_data": processed}

# Chain com fun√ß√£o customizada
chain = process_data | template | model
result = chain.invoke({"data": "hello world"})
```

---

## ‚ö° Script 3: 2-chains-e-processamento/3-runnable-lambda.py

### Explica√ß√£o do RunnableLambda:
```python
def parse_number(text:str) -> int:
    return int(text.strip())

parse_runnable = RunnableLambda(parse_number)
```

**O que √© RunnableLambda:**
- **Transforma fun√ß√µes Python simples** em componentes LCEL
- **Alternativa mais simples** ao decorador @chain
- **Permite tipos diretos** (n√£o precisa de dicion√°rios)

### Diferen√ßa entre @chain e RunnableLambda:

**@chain (decorador):**
```python
@chain
def square(input_dict:dict) -> dict:
    x = input_dict["x"]
    return {"square_result": x * x}  # SEMPRE dicion√°rio
```

**RunnableLambda:**
```python
def parse_number(text:str) -> int:
    return int(text.strip())  # Pode retornar qualquer tipo

parse_runnable = RunnableLambda(parse_number)
```

### Quando usar cada um:

**Use @chain quando:**
- ‚úÖ Integrando com outros componentes LangChain
- ‚úÖ Precisando passar dados entre componentes
- ‚úÖ Trabalhando em pipelines complexos

**Use RunnableLambda quando:**
- ‚úÖ Fun√ß√£o simples com tipos diretos
- ‚úÖ Processamento b√°sico de dados
- ‚úÖ N√£o precisa de entrada/sa√≠da como dicion√°rio

### Vantagens do RunnableLambda:
- **Simplicidade**: N√£o precisa de dicion√°rios
- **Flexibilidade**: Pode usar qualquer tipo de entrada/sa√≠da
- **Legibilidade**: C√≥digo mais direto e simples
- **Reutiliza√ß√£o**: Fun√ß√µes Python existentes podem ser facilmente convertidas

### Exemplo de uso:
```python
# Fun√ß√£o simples
def clean_text(text: str) -> str:
    return text.strip().lower()

# Transforma em componente
clean_runnable = RunnableLambda(clean_text)

# Usa em chain
chain = clean_runnable | template | model
result = chain.invoke("  HELLO WORLD  ")
```

---

## üîÑ Script 4: 2-chains-e-processamento/4-pipeline-de-processamento.py

### Explica√ß√£o do StrOutputParser:
```python
from langchain_core.output_parsers import StrOutputParser

translate = template_translate | llm_en | StrOutputParser()
```

**O que √© StrOutputParser:**
- **Extrai apenas o texto** das respostas dos modelos
- **Remove metadados** e informa√ß√µes extras
- **Retorna string pura** em vez de objeto complexo

### Explica√ß√£o do pipeline complexo:
```python
pipeline = {"text": translate} | template_summary | llm_en | StrOutputParser()
```

**Estrutura do pipeline:**
1. **`{"text": translate}`**: Mapeia a sa√≠da da tradu√ß√£o para a vari√°vel "text"
2. **`template_summary`**: Usa o texto traduzido para criar prompt de sumariza√ß√£o
3. **`llm_en`**: Gera a sumariza√ß√£o
4. **`StrOutputParser()`**: Extrai apenas o texto da resposta

### Fluxo de execu√ß√£o:
```python
# 1. Entrada: "LangChain √© um framework para desenvolvimento de aplica√ß√µes de IA"
# 2. Tradu√ß√£o: "LangChain is a framework for AI application development"
# 3. Sumariza√ß√£o: "AI framework for development"
# 4. Sa√≠da: "AI framework for development"
```

### Vantagens dos pipelines complexos:
- **Processamento sequencial**: M√∫ltiplas etapas em uma √∫nica chain
- **Reutiliza√ß√£o**: Cada etapa pode ser usada independentemente
- **Modularidade**: F√°cil de modificar ou adicionar etapas
- **Efici√™ncia**: Processamento otimizado em sequ√™ncia

### Exemplo de uso do StrOutputParser:
```python
# Sem StrOutputParser:
response = model.invoke("Hello")
# Resultado: AIMessage(content="Hello there!", additional_kwargs={}, ...)

# Com StrOutputParser:
chain = model | StrOutputParser()
response = chain.invoke("Hello")
# Resultado: "Hello there!"
```

### Padr√£o de mapeamento com dicion√°rio:
```python
# Mapeia sa√≠da de uma chain para entrada de outra
pipeline = {"variavel": chain_anterior} | proxima_chain

# Exemplo:
chain1 = template1 | model1 | StrOutputParser()
chain2 = template2 | model2 | StrOutputParser()
pipeline = {"texto_processado": chain1} | chain2
```

---

## üìù Script 5: 2-chains-e-processamento/5-sumarizacao.py

### Explica√ß√£o do RecursiveCharacterTextSplitter:
```python
splitter = RecursiveCharacterTextSplitter(
    chunk_size=250, chunk_overlap=70, 
)
parts = splitter.create_documents([long_text])
```

**O que √© RecursiveCharacterTextSplitter:**
- **Divide textos longos** em chunks menores
- **Mant√©m contexto** entre chunks com sobreposi√ß√£o
- **Permite processamento** de textos que excedem o limite dos modelos

**Par√¢metros importantes:**
- **`chunk_size=250`**: Tamanho m√°ximo de cada chunk (caracteres)
- **`chunk_overlap=70`**: Sobreposi√ß√£o entre chunks para manter contexto
- **`create_documents()`**: Converte texto em lista de documentos

### Explica√ß√£o da t√©cnica "stuff":
```python
chain_sumarize = load_summarize_chain(llm, chain_type="stuff", verbose=False)
```

**O que √© a t√©cnica "stuff":**
- **Coloca todo o texto** em um √∫nico prompt
- **Funciona bem** para textos que cabem no contexto do modelo
- **Processamento simples** e direto

**Vantagens:**
- ‚úÖ Simples de implementar
- ‚úÖ Mant√©m contexto completo
- ‚úÖ R√°pido para textos menores

**Limita√ß√µes:**
- ‚ùå Limitado pelo tamanho do contexto do modelo
- ‚ùå Pode ser ineficiente para textos muito longos

### Fluxo de processamento:
```python
# 1. Texto longo ‚Üí RecursiveCharacterTextSplitter
# 2. Chunks menores ‚Üí load_summarize_chain
# 3. T√©cnica "stuff" ‚Üí Resumo final
```

### Exemplo de uso:
```python
# Dividir texto
splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=70)
parts = splitter.create_documents([texto_longo])

# Sumarizar
chain = load_summarize_chain(llm, chain_type="stuff")
result = chain.invoke({"input_documents": parts})
resumo = result["output_text"]
```

### Quando usar a t√©cnica "stuff":
- ‚úÖ Textos que cabem no contexto do modelo
- ‚úÖ Sumariza√ß√µes simples e diretas
- ‚úÖ Quando voc√™ quer manter todo o contexto
- ‚úÖ Processamento r√°pido

---

## üîÑ Script 6: 2-chains-e-processamento/6-sumarizacao-com-map-reduce.py

### Explica√ß√£o da t√©cnica "map_reduce":
```python
chain_sumarize = load_summarize_chain(llm, chain_type="map_reduce", verbose=False)
```

**O que √© a t√©cnica "map_reduce":**
- **Processa cada chunk separadamente** (fase "map")
- **Combina os resumos** em um resumo final (fase "reduce")
- **Funciona com textos muito longos** que n√£o cabem no contexto

### Diferen√ßa detalhada entre "stuff" e "map_reduce":

**T√©cnica "stuff" (Script 5):**
```python
chain_type="stuff"
```
- **Processamento**: Coloca TODO o texto em um √∫nico prompt
- **Limita√ß√£o**: Limitado pelo tamanho do contexto do modelo
- **Velocidade**: Mais r√°pido (uma √∫nica chamada)
- **Contexto**: Mant√©m contexto completo
- **Uso**: Textos que cabem no contexto

**T√©cnica "map_reduce" (Script 6):**
```python
chain_type="map_reduce"
```
- **Processamento**: 
  1. **Map**: Sumariza cada chunk separadamente
  2. **Reduce**: Combina todos os resumos em um resumo final
- **Limita√ß√£o**: N√£o tem limite de tamanho (processa por partes)
- **Velocidade**: Mais lento (m√∫ltiplas chamadas)
- **Contexto**: Pode perder contexto entre chunks
- **Uso**: Textos muito longos

### Fluxo de processamento map_reduce:
```python
# Fase 1 - MAP: Processa cada chunk
Chunk 1 ‚Üí Sumariza√ß√£o ‚Üí Resumo 1
Chunk 2 ‚Üí Sumariza√ß√£o ‚Üí Resumo 2
Chunk 3 ‚Üí Sumariza√ß√£o ‚Üí Resumo 3
...

# Fase 2 - REDUCE: Combina os resumos
[Resumo 1, Resumo 2, Resumo 3, ...] ‚Üí Sumariza√ß√£o Final ‚Üí Resumo Final
```

### Compara√ß√£o pr√°tica:

**"Stuff" (Script 5):**
```python
# Uma √∫nica chamada
Texto completo ‚Üí Modelo ‚Üí Resumo
```

**"Map_reduce" (Script 6):**
```python
# M√∫ltiplas chamadas
Chunk 1 ‚Üí Modelo ‚Üí Resumo 1
Chunk 2 ‚Üí Modelo ‚Üí Resumo 2
...
[Resumos] ‚Üí Modelo ‚Üí Resumo Final
```

### Quando usar cada t√©cnica:

**Use "stuff" quando:**
- ‚úÖ Texto cabe no contexto do modelo
- ‚úÖ Quer processamento r√°pido
- ‚úÖ Precisa manter contexto completo
- ‚úÖ Textos at√© ~4000 tokens

**Use "map_reduce" quando:**
- ‚úÖ Texto √© muito longo (excede contexto)
- ‚úÖ Pode aceitar perda de contexto entre partes
- ‚úÖ Precisa processar documentos grandes
- ‚úÖ Textos com mais de ~4000 tokens

### Vantagens e desvantagens:

**"Stuff":**
- ‚úÖ R√°pido, mant√©m contexto, simples
- ‚ùå Limitado pelo contexto do modelo

**"Map_reduce":**
- ‚úÖ Funciona com textos ilimitados, escal√°vel
- ‚ùå Mais lento, pode perder contexto, complexo

---

## üîß Script 7: 2-chains-e-processamento/7-pipeline-de-sumarizacao.py

### Explica√ß√£o do pipeline manual map-reduce:
```python
# Fase MAP
map_prompt = PromptTemplate.from_template("Write a concise summary of the following text:\n{context}")
map_chain = map_prompt | llm | StrOutputParser()
prepare_map_inputs = RunnableLambda(lambda docs: [{"context": d.page_content} for d in docs])
map_stage = prepare_map_inputs | map_chain.map()

# Fase REDUCE
reduce_prompt = PromptTemplate.from_template("Combine the following summaries into a single concise summary:\n{context}")
reduce_chain = reduce_prompt | llm | StrOutputParser()
prepare_reduce_input = RunnableLambda(lambda summaries: {"context": "\n".join(summaries)})

# Pipeline completo
pipeline = map_stage | prepare_reduce_input | reduce_chain
```

### Diferen√ßa entre pipeline manual e load_summarize_chain:

**load_summarize_chain (Script 6):**
```python
chain = load_summarize_chain(llm, chain_type="map_reduce")
```
- ‚úÖ Mais simples de implementar
- ‚ùå Menos controle sobre cada etapa
- ‚ùå Prompts fixos (n√£o personaliz√°veis)

**Pipeline manual (Script 7):**
```python
# Controle total sobre cada etapa
map_prompt = PromptTemplate.from_template("Seu prompt personalizado")
reduce_prompt = PromptTemplate.from_template("Seu prompt personalizado")
```
- ‚úÖ Controle total sobre prompts
- ‚úÖ Personaliza√ß√£o completa
- ‚úÖ Flexibilidade m√°xima
- ‚ùå Mais c√≥digo para implementar

### Explica√ß√£o do PromptTemplate.from_template():
```python
map_prompt = PromptTemplate.from_template("Write a concise summary of the following text:\n{context}")
```

**Vantagem:**
- **Cria√ß√£o r√°pida** de templates simples
- **N√£o precisa definir** input_variables explicitamente
- **Detecta automaticamente** as vari√°veis no template

### Explica√ß√£o do m√©todo .map():
```python
map_stage = prepare_map_inputs | map_chain.map()
```

**O que faz:**
- **Aplica a chain** a cada item da lista de inputs
- **Processamento paralelo** de m√∫ltiplos documentos
- **Retorna lista** de resultados

### Fluxo detalhado do pipeline:

**Fase MAP:**
```python
# 1. prepare_map_inputs: Converte documentos em lista de dicion√°rios
# 2. map_chain.map(): Aplica sumariza√ß√£o a cada chunk
# 3. Resultado: Lista de resumos individuais
```

**Fase REDUCE:**
```python
# 1. prepare_reduce_input: Junta todos os resumos em uma string
# 2. reduce_chain: Combina os resumos em um resumo final
# 3. Resultado: Resumo final consolidado
```

### Vantagens do pipeline manual:
- **Controle total**: Personaliza cada etapa
- **Prompts customizados**: Adapta para seu caso de uso
- **Flexibilidade**: Pode adicionar etapas intermedi√°rias
- **Debugging**: F√°cil de identificar problemas
- **Otimiza√ß√£o**: Pode ajustar cada componente

### Exemplo de personaliza√ß√£o:
```python
# Prompt personalizado para sumariza√ß√£o
map_prompt = PromptTemplate.from_template(
    "Summarize this text in exactly 3 bullet points:\n{context}"
)

# Prompt personalizado para combina√ß√£o
reduce_prompt = PromptTemplate.from_template(
    "Create a poetic summary of these summaries:\n{context}"
)
```

---

# ü§ñ PASTA 3: AGENTES E TOOLS

---

## ü§ñ Script 1: 3-agentes-e-tools/1-agente-react-e-tools.py

### Explica√ß√£o do Agente ReAct:
```python
from langchain.agents import create_react_agent, AgentExecutor
```

**O que √© um Agente ReAct:**
- **ReAct**: Reasoning and Acting (Racioc√≠nio e A√ß√£o)
- **Raciocina** sobre qual ferramenta usar
- **Executa a√ß√µes** para obter informa√ß√µes
- **Combina** pensamento e a√ß√£o

### Explica√ß√£o do decorador @tool:
```python
@tool("calculator", return_direct=True)
def calculator(expression: str) -> str:
    """Evaluate a simple mathematical expression and return the result as a string."""
    return str(result)
```

**O que √© o decorador @tool:**
- **Transforma fun√ß√µes Python** em ferramentas do agente
- **`return_direct=True`**: Retorna resultado diretamente
- **`return_direct=False`**: Permite processamento adicional pelo agente

### Explica√ß√£o do formato ReAct:
```python
Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [tools]
Action Input: the input to the action
Observation: the result of the action
```

**Fluxo de racioc√≠nio:**
1. **Question**: Pergunta original
2. **Thought**: Agente pensa sobre o que fazer
3. **Action**: Escolhe qual ferramenta usar
4. **Action Input**: Par√¢metros para a ferramenta
5. **Observation**: Resultado da ferramenta
6. **Repete** at√© ter a resposta final

### Explica√ß√£o do AgentExecutor:
```python
agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent_chain, 
    tools=tools, 
    verbose=True, 
    handle_parsing_errors="Invalid format...",
    max_iterations=3
)
```

**Par√¢metros importantes:**
- **`verbose=True`**: Mostra o processo de racioc√≠nio
- **`max_iterations=3`**: Limita tentativas (evita loops infinitos)
- **`handle_parsing_errors`**: Mensagem para erros de formato

### Vantagens dos Agentes ReAct:
- **Racioc√≠nio expl√≠cito**: Pode ver como o agente pensa
- **Flexibilidade**: Pode usar m√∫ltiplas ferramentas
- **Extensibilidade**: F√°cil adicionar novas ferramentas
- **Controle**: Define limites e regras

### Exemplo de uso:
```python
# Criar ferramenta
@tool("search")
def search(query: str) -> str:
    return "Resultado da busca"

# Criar agente
tools = [search]
agent = create_react_agent(llm, tools, prompt)
executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools)

# Usar agente
result = executor.invoke({"input": "Pergunta do usu√°rio"})
```

### Diferen√ßa entre Tools e Fun√ß√µes normais:
- **Fun√ß√£o normal**: Executa diretamente
- **Tool**: Integrada ao agente, pode ser escolhida dinamicamente
- **Tool**: Tem descri√ß√£o que o agente usa para decidir

---

## üîó Script 2: 3-agentes-e-tools/2-agente-react-usando-prompt-hub.py

### Explica√ß√£o do Prompt Hub:
```python
from langchain import hub
prompt = hub.pull("hwchase17/react")
```

**O que √© o Prompt Hub:**
- **Reposit√≥rio de prompts** da comunidade LangChain
- **Prompts testados** e otimizados
- **Reutiliza√ß√£o** de templates comprovados
- **Padr√µes** estabelecidos pela comunidade

### Diferen√ßa entre prompt personalizado e Prompt Hub:

**Prompt personalizado (Script 1):**
```python
prompt = PromptTemplate.from_template("""
Answer the following questions as best you can...
{tools}
Use the following format:
Question: {input}
Thought: {agent_scratchpad}
""")
```
- ‚úÖ Controle total sobre o prompt
- ‚ùå Precisa escrever do zero
- ‚ùå Pode ter bugs ou inefici√™ncias

**Prompt Hub (Script 2):**
```python
prompt = hub.pull("hwchase17/react")
```
- ‚úÖ Prompt testado e otimizado
- ‚úÖ Criado por especialistas
- ‚úÖ Padr√£o da comunidade
- ‚ùå Menos flexibilidade

### Explica√ß√£o do "hwchase17/react":
```python
hub.pull("hwchase17/react")
```

**O que √©:**
- **"hwchase17"**: Harrison Chase (criador do LangChain)
- **"react"**: Prompt para agentes ReAct
- **Prompt oficial** do LangChain para ReAct
- **Testado extensivamente** pela comunidade

### Vantagens do Prompt Hub:
- **Qualidade**: Prompts testados e otimizados
- **Padroniza√ß√£o**: Usa formatos estabelecidos
- **Rapidez**: N√£o precisa criar prompts do zero
- **Confiabilidade**: Menos bugs e problemas
- **Comunidade**: Beneficia-se de melhorias coletivas

### Exemplo de outros prompts do hub:
```python
# Diferentes tipos de prompts dispon√≠veis
prompt1 = hub.pull("hwchase17/react")           # Agente ReAct
prompt2 = hub.pull("hwchase17/react-chat")      # ReAct para chat
prompt3 = hub.pull("hwchase17/zero-shot-react") # ReAct zero-shot
```

### Quando usar cada abordagem:

**Use Prompt Hub quando:**
- ‚úÖ Quer um prompt testado e confi√°vel
- ‚úÖ Est√° come√ßando com agentes
- ‚úÖ Precisa de padr√µes estabelecidos
- ‚úÖ Quer rapidez na implementa√ß√£o

**Use prompt personalizado quando:**
- ‚úÖ Precisa de controle total
- ‚úÖ Tem requisitos espec√≠ficos
- ‚úÖ Quer otimizar para seu caso de uso
- ‚úÖ Precisa de funcionalidades customizadas

### Fluxo de uso do Prompt Hub:
```python
# 1. Importar hub
from langchain import hub

# 2. Baixar prompt
prompt = hub.pull("hwchase17/react")

# 3. Usar no agente
agent = create_react_agent(llm, tools, prompt)
```

---

# üß† PASTA 4: GERENCIAMENTO DE MEM√ìRIA

---

## üß† Script 1: 4-gerenciamento-de-memoria/1-armazenamento-de-historico.py

### Explica√ß√£o do MessagesPlaceholder:
```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}"),
])
```

**O que √© MessagesPlaceholder:**
- **Reserva espa√ßo** no prompt para inserir mensagens do hist√≥rico
- **Vari√°vel din√¢mica** que √© preenchida com mensagens anteriores
- **Mant√©m contexto** entre m√∫ltiplas intera√ß√µes

### Explica√ß√£o do InMemoryChatMessageHistory:
```python
from langchain_core.chat_history import InMemoryChatMessageHistory

session_store: dict[str, InMemoryChatMessageHistory] = {}
```

**O que √© InMemoryChatMessageHistory:**
- **Armazena hist√≥rico** de mensagens em mem√≥ria
- **Tempor√°rio**: Perdido quando aplica√ß√£o √© fechada
- **R√°pido**: Acesso direto na mem√≥ria RAM
- **Simples**: Ideal para testes e desenvolvimento

### Explica√ß√£o do RunnableWithMessageHistory:
```python
from langchain_core.runnables import RunnableWithMessageHistory

conversational_chain = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history",
)
```

**Par√¢metros importantes:**
- **`chain`**: Chain base (prompt + modelo)
- **`get_session_history`**: Fun√ß√£o para obter hist√≥rico da sess√£o
- **`input_messages_key="input"`**: Chave para mensagem atual
- **`history_messages_key="history"`**: Chave para hist√≥rico no prompt

### Explica√ß√£o do gerenciamento de sess√µes:
```python
def get_session_history(session_id: str) -> InMemoryChatMessageHistory:
    if session_id not in session_store:
        session_store[session_id] = InMemoryChatMessageHistory()
    return session_store[session_id]

config = {"configurable": {"session_id": "demo-session"}}
```

**Como funciona:**
- **session_id**: Identifica unicamente uma conversa
- **session_store**: Dicion√°rio que armazena hist√≥ricos por sess√£o
- **get_session_history**: Cria ou recupera hist√≥rico da sess√£o
- **config**: Configura√ß√£o passada para identificar a sess√£o

### Fluxo de funcionamento:
```python
# 1. Primeira mensagem
response1 = conversational_chain.invoke({"input": "Hello, my name is Wesley"}, config=config)
# Hist√≥rico: [HumanMessage("Hello, my name is Wesley"), AIMessage("Hi Wesley!")]

# 2. Segunda mensagem
response2 = conversational_chain.invoke({"input": "What's my name?"}, config=config)
# Hist√≥rico: [mensagens anteriores + HumanMessage("What's my name?"), AIMessage("Your name is Wesley")]
```

### Vantagens da mem√≥ria de conversa:
- **Contexto cont√≠nuo**: Modelo lembra informa√ß√µes anteriores
- **Conversas naturais**: Pode referenciar mensagens passadas
- **M√∫ltiplas sess√µes**: Diferentes usu√°rios/conversas isoladas
- **Flexibilidade**: Pode ser implementada de v√°rias formas

### Limita√ß√µes do InMemoryChatMessageHistory:
- **Tempor√°rio**: Perdido ao reiniciar aplica√ß√£o
- **Limitado**: Pode consumir muita mem√≥ria com conversas longas
- **Sem persist√™ncia**: N√£o salva em banco de dados

### Exemplo de uso pr√°tico:
```python
# Configurar mem√≥ria
session_store = {}
def get_session_history(session_id: str):
    if session_id not in session_store:
        session_store[session_id] = InMemoryChatMessageHistory()
    return session_store[session_id]

# Criar chain com mem√≥ria
chain_with_memory = RunnableWithMessageHistory(
    chain, get_session_history,
    input_messages_key="input",
    history_messages_key="history"
)

# Usar com diferentes sess√µes
config1 = {"configurable": {"session_id": "user1"}}
config2 = {"configurable": {"session_id": "user2"}}
```

---

## üß† Script 2: 4-gerenciamento-de-memoria/2-historico-baseado-em-sliding-window.py

### Explica√ß√£o do trim_messages:
```python
from langchain_core.messages import trim_messages

trimmed = trim_messages(
    raw_history,
    token_counter=len,
    max_tokens=2,
    strategy="last",
    start_on="human",
    include_system=True,
    allow_partial=False,
)
```

**O que √© trim_messages:**
- **Controla tamanho** do hist√≥rico de mensagens
- **Reduz automaticamente** para caber no limite de tokens
- **Implementa sliding window** para manter contexto recente
- **Evita overflow** de contexto do modelo

### Par√¢metros importantes:

**`token_counter=len`:**
- **Fun√ß√£o para contar** tokens/mensagens
- **`len`**: Conta caracteres (simples para demonstra√ß√£o)
- **Em produ√ß√£o**: Use tokenizer real do modelo

**`max_tokens=2`:**
- **Limite m√°ximo** de tokens/mensagens
- **Muito baixo** neste exemplo para demonstrar o efeito
- **Em produ√ß√£o**: Use limite real do modelo (ex: 4000)

**`strategy="last"`:**
- **Estrat√©gia de corte**: manter as √∫ltimas mensagens
- **Alternativas**: `"first"` (primeiras), `"middle"` (meio)
- **Mais comum**: `"last"` para manter contexto recente

**`start_on="human"`:**
- **Ponto de in√≠cio** para contar tokens
- **`"human"`**: Come√ßa a contar de mensagens humanas
- **Alternativas**: `"assistant"`, `"system"`

**`include_system=True`:**
- **Inclui mensagem** do sistema no hist√≥rico
- **Importante**: Manter instru√ß√µes do sistema
- **Sempre True**: Para manter comportamento do assistente

**`allow_partial=False`:**
- **N√£o corta** mensagens no meio
- **Mant√©m integridade** das mensagens
- **Mais seguro**: Evita contexto incompleto

### Explica√ß√£o do Sliding Window:
```python
def prepare_inputs(payload: dict) -> dict:
    raw_history = payload.get("raw_history", [])
    trimmed = trim_messages(raw_history, ...)
    return {"input": payload.get("input",""), "history": trimmed}
```

**Como funciona:**
- **Hist√≥rico bruto**: Todas as mensagens da conversa
- **Processamento**: `trim_messages` reduz para limite
- **Sliding window**: Mant√©m apenas mensagens recentes
- **Resultado**: Modelo "esquece" informa√ß√µes antigas

### Diferen√ßa entre raw_history e history:
```python
conversational_chain = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="raw_history"  # Hist√≥rico bruto
)
```

**raw_history vs history:**
- **`raw_history`**: Hist√≥rico completo (antes do trim)
- **`history`**: Hist√≥rico processado (depois do trim)
- **Fluxo**: raw_history ‚Üí trim_messages ‚Üí history ‚Üí modelo

### Fluxo completo do Sliding Window:

**1. Armazenamento:**
```python
# Todas as mensagens s√£o salvas em raw_history
session_store[session_id] = InMemoryChatMessageHistory()
```

**2. Processamento:**
```python
# trim_messages reduz o hist√≥rico para caber no limite
trimmed = trim_messages(raw_history, max_tokens=2, strategy="last")
```

**3. Envio ao modelo:**
```python
# Modelo recebe apenas as mensagens mais recentes
chain.invoke({"input": "nova mensagem", "history": trimmed})
```

### Vantagens do Sliding Window:
- **Controle de custos**: Limita tokens enviados ao modelo
- **Performance**: Evita contextos muito longos
- **Conformidade**: Respeita limites de API
- **Flexibilidade**: Diferentes estrat√©gias de corte

### Limita√ß√µes do Sliding Window:
- **Perda de contexto**: Informa√ß√µes antigas s√£o perdidas
- **Configura√ß√£o complexa**: Precisa ajustar par√¢metros
- **Token counting**: Precisa de tokenizer preciso
- **Estrat√©gia de corte**: Pode perder informa√ß√µes importantes

### Exemplo pr√°tico de uso:
```python
# Configura√ß√£o realista
trimmed = trim_messages(
    raw_history,
    token_counter=tokenizer.count_tokens,  # Tokenizer real
    max_tokens=4000,  # Limite do modelo
    strategy="last",
    start_on="human",
    include_system=True,
    allow_partial=False,
)
```

### Compara√ß√£o com mem√≥ria simples:

**Mem√≥ria simples (Script 1):**
```python
# Mant√©m todo o hist√≥rico
history_messages_key="history"
```
- ‚úÖ Contexto completo
- ‚ùå Pode exceder limites
- ‚ùå Custo crescente

**Sliding Window (Script 2):**
```python
# Controla tamanho do hist√≥rico
history_messages_key="raw_history"
prepare_inputs = RunnableLambda(lambda p: trim_messages(p["raw_history"]))
```
- ‚úÖ Controle de custos
- ‚úÖ Performance consistente
- ‚ùå Perda de contexto antigo

---

# üìö PASTA 5: LOADERS E BANCO DE DADOS VETORIAIS

---

## üìö Script 1: 5-loaders-e-banco-de-dados-vetoriais/1-carregamento-usando-WebBaseLoader copy.py

### Explica√ß√£o do WebBaseLoader:
```python
from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://www.langchain.com/")
docs = loader.load()
```

**O que √© WebBaseLoader:**
- **Carrega conte√∫do** de p√°ginas web via HTTP
- **Extrai texto limpo** removendo HTML e formata√ß√£o
- **Suporta m√∫ltiplas URLs** em uma √∫nica requisi√ß√£o
- **Automatiza** o processo de coleta de dados da web

### Explica√ß√£o do m√©todo load():
```python
docs = loader.load()
```

**O que faz:**
- **Faz requisi√ß√£o HTTP** para a URL especificada
- **Extrai conte√∫do** da p√°gina web
- **Remove HTML** e formata√ß√£o desnecess√°ria
- **Retorna lista** de documentos (Document objects)

### Explica√ß√£o do split_documents():
```python
chunks = splitter.split_documents(docs)
```

**O que faz:**
- **Recebe lista** de documentos
- **Aplica splitter** a cada documento
- **Retorna chunks** menores e gerenci√°veis
- **Mant√©m metadados** dos documentos originais

### Fluxo completo de carregamento:

**1. Carregamento:**
```python
loader = WebBaseLoader("https://www.langchain.com/")
docs = loader.load()
# Resultado: Lista de Document objects com conte√∫do da p√°gina
```

**2. Divis√£o em chunks:**
```python
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
chunks = splitter.split_documents(docs)
# Resultado: Lista de chunks menores para processamento
```

**3. Processamento:**
```python
for chunk in chunks:
    print(chunk)  # Cada chunk √© um Document object
```

### Estrutura do Document object:
```python
# Cada documento tem:
chunk.page_content  # Conte√∫do do texto
chunk.metadata      # Metadados (URL, t√≠tulo, etc.)
```

### Vantagens do WebBaseLoader:
- **Simplicidade**: Uma linha para carregar conte√∫do web
- **Automa√ß√£o**: Remove necessidade de scraping manual
- **Robustez**: Lida com diferentes tipos de p√°ginas
- **Flexibilidade**: Suporta m√∫ltiplas URLs

### Limita√ß√µes do WebBaseLoader:
- **Depend√™ncia de internet**: Requer conex√£o ativa
- **Rate limiting**: Pode ser bloqueado por sites
- **Conte√∫do din√¢mico**: N√£o carrega JavaScript
- **Formata√ß√£o**: Pode perder estrutura complexa

### Exemplo de uso com m√∫ltiplas URLs:
```python
# Carregar m√∫ltiplas p√°ginas
urls = [
    "https://www.langchain.com/",
    "https://www.langchain.com/docs/",
    "https://www.langchain.com/community/"
]
loader = WebBaseLoader(urls)
docs = loader.load()
```

### Exemplo de configura√ß√£o avan√ßada:
```python
# Configurar headers personalizados
loader = WebBaseLoader(
    "https://www.langchain.com/",
    requests_kwargs={
        "headers": {
            "User-Agent": "Mozilla/5.0 (compatible; MyBot/1.0)"
        }
    }
)
```

---

## üìö Script 2: 5-loaders-e-banco-de-dados-vetoriais/2-carregamento-de-pdf.py

### Explica√ß√£o do PyPDFLoader:
```python
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("./gpt5.pdf")
docs = loader.load()
```

**O que √© PyPDFLoader:**
- **Carrega arquivos PDF** locais do sistema
- **Extrai texto** de todas as p√°ginas do PDF
- **Preserva estrutura** b√°sica do documento
- **Suporta PDFs** com texto e imagens (OCR n√£o inclu√≠do)

### Explica√ß√£o do m√©todo load() para PDFs:
```python
docs = loader.load()
```

**O que faz:**
- **L√™ arquivo PDF** do caminho especificado
- **Extrai texto** de cada p√°gina
- **Cria Document objects** para cada p√°gina
- **Inclui metadados** como n√∫mero da p√°gina

### Estrutura dos documentos PDF:
```python
# Cada documento representa uma p√°gina do PDF
for doc in docs:
    print(f"P√°gina {doc.metadata['page']}: {doc.page_content[:100]}...")
```

**Metadados t√≠picos:**
- **`page`**: N√∫mero da p√°gina
- **`source`**: Caminho do arquivo PDF
- **`total_pages`**: Total de p√°ginas no PDF

### Fluxo completo de carregamento PDF:

**1. Carregamento:**
```python
loader = PyPDFLoader("./gpt5.pdf")
docs = loader.load()
# Resultado: Lista de Document objects (um por p√°gina)
```

**2. Divis√£o em chunks:**
```python
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
chunks = splitter.split_documents(docs)
# Resultado: Chunks menores que podem cruzar p√°ginas
```

**3. An√°lise:**
```python
print(len(chunks))  # N√∫mero total de chunks criados
```

### Vantagens do PyPDFLoader:
- **Simplicidade**: Uma linha para carregar PDFs
- **Preserva√ß√£o**: Mant√©m estrutura de p√°ginas
- **Metadados**: Inclui informa√ß√µes da p√°gina
- **Compatibilidade**: Funciona com maioria dos PDFs

### Limita√ß√µes do PyPDFLoader:
- **PDFs escaneados**: N√£o extrai texto de imagens
- **Formata√ß√£o complexa**: Pode perder layout
- **Tabelas**: Pode n√£o preservar estrutura
- **Arquivos corrompidos**: Pode falhar

### Exemplo de uso com metadados:
```python
# Acessar metadados dos documentos
for doc in docs:
    print(f"P√°gina {doc.metadata['page']}")
    print(f"Conte√∫do: {doc.page_content[:200]}...")
    print("-" * 50)
```

### Exemplo de configura√ß√£o avan√ßada:
```python
# Carregar apenas p√°ginas espec√≠ficas
loader = PyPDFLoader("./gpt5.pdf")
docs = loader.load()

# Filtrar p√°ginas espec√≠ficas
filtered_docs = [doc for doc in docs if doc.metadata['page'] <= 5]
```

### Compara√ß√£o entre loaders:

**WebBaseLoader (Script 1):**
```python
loader = WebBaseLoader("https://www.langchain.com/")
```
- ‚úÖ Carrega conte√∫do web
- ‚ùå Requer internet
- ‚ùå Pode ser bloqueado

**PyPDFLoader (Script 2):**
```python
loader = PyPDFLoader("./gpt5.pdf")
```
- ‚úÖ Carrega arquivos locais
- ‚úÖ Funciona offline
- ‚úÖ Preserva metadados de p√°gina

### Dicas importantes:
- **Caminho relativo**: Use `"./arquivo.pdf"` para arquivos no diret√≥rio atual
- **Caminho absoluto**: Use `/caminho/completo/arquivo.pdf` para arquivos espec√≠ficos
- **Verifica√ß√£o**: Sempre verifique se o arquivo existe antes de carregar
- **Tamanho**: PDFs muito grandes podem demorar para carregar

---

## üìö Script 3: 5-loaders-e-banco-de-dados-vetoriais/3-ingestion-pgvector.py

### Explica√ß√£o do OpenAIEmbeddings:
```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model=os.getenv("OPENAI_MODEL","text-embedding-3-small"))
```

**O que √© OpenAIEmbeddings:**
- **Cria vetores num√©ricos** dos textos (embeddings)
- **Converte texto** em representa√ß√£o matem√°tica
- **Permite busca sem√¢ntica** por similaridade
- **Modelo padr√£o**: "text-embedding-3-small" (1536 dimens√µes)

### Explica√ß√£o do PGVector:
```python
from langchain_postgres import PGVector

store = PGVector(
    embeddings=embeddings,
    collection_name=os.getenv("PGVECTOR_COLLECTION"),
    connection=os.getenv("PGVECTOR_URL"),
    use_jsonb=True,
)
```

**O que √© PGVector:**
- **Armazena vetores** no PostgreSQL com extens√£o pgvector
- **Permite busca por similaridade** usando dist√¢ncia vetorial
- **Suporta metadados** em formato JSONB
- **Escal√°vel**: PostgreSQL √© robusto para grandes volumes

### Par√¢metros do PGVector:

**`embeddings`:**
- **Modelo de embeddings** para criar vetores
- **Deve ser consistente** entre ingest√£o e busca

**`collection_name`:**
- **Nome da cole√ß√£o** no banco de dados
- **Organiza documentos** em grupos l√≥gicos

**`connection`:**
- **URL de conex√£o** com PostgreSQL
- **Formato**: `postgresql://user:password@host:port/database`

**`use_jsonb=True`:**
- **Usa JSONB** para metadados (mais eficiente)
- **Melhor performance** que JSON simples

### Explica√ß√£o da verifica√ß√£o de vari√°veis:
```python
for k in ("OPENAI_API_KEY", "PGVECTOR_URL","PGVECTOR_COLLECTION"):
    if not os.getenv(k):
        raise RuntimeError(f"Environment variable {k} is not set")
```

**O que faz:**
- **Valida configura√ß√£o** antes de executar
- **Evita erros** em tempo de execu√ß√£o
- **Garante** que todas as vari√°veis necess√°rias est√£o definidas

### Explica√ß√£o do Path(__file__).parent:
```python
current_dir = Path(__file__).parent
pdf_path = current_dir / "gpt5.pdf"
```

**O que faz:**
- **`__file__`**: Caminho do script atual
- **`.parent`**: Diret√≥rio pai (onde est√° o script)
- **Constr√≥i caminho** relativo para o PDF
- **Port√°vel**: Funciona em diferentes sistemas

### Explica√ß√£o da limpeza de metadados:
```python
enriched = [
    Document(
        page_content=d.page_content,
        metadata={k: v for k, v in d.metadata.items() if v not in ("", None)}
    )
    for d in splits
]
```

**O que faz:**
- **Remove valores vazios** (`""`) e nulos (`None`)
- **Limpa metadados** desnecess√°rios
- **Reduz tamanho** do banco de dados
- **Melhora performance** de busca

### Explica√ß√£o da gera√ß√£o de IDs:
```python
ids = [f"doc-{i}" for i in range(len(enriched))]
```

**O que faz:**
- **Cria IDs √∫nicos** para cada documento
- **Formato**: "doc-0", "doc-1", "doc-2", etc.
- **Permite atualiza√ß√£o** e remo√ß√£o espec√≠fica
- **Facilita debugging** e rastreamento

### Explica√ß√£o do add_documents():
```python
store.add_documents(documents=enriched, ids=ids)
```

**O que faz:**
- **Armazena documentos** no banco vetorial
- **Cria embeddings** automaticamente
- **Associa IDs** aos documentos
- **Indexa** para busca r√°pida

### Fluxo completo de ingest√£o:

**1. Carregamento:**
```python
docs = PyPDFLoader(str(pdf_path)).load()
# Resultado: Document objects do PDF
```

**2. Divis√£o:**
```python
splits = RecursiveCharacterTextSplitter(...).split_documents(docs)
# Resultado: Chunks menores
```

**3. Limpeza:**
```python
enriched = [Document(...) for d in splits]
# Resultado: Documentos limpos
```

**4. Embeddings:**
```python
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
# Resultado: Modelo para criar vetores
```

**5. Armazenamento:**
```python
store.add_documents(documents=enriched, ids=ids)
# Resultado: Documentos indexados no banco
```

### Vari√°veis de ambiente necess√°rias:

**`.env`:**
```bash
OPENAI_API_KEY=sk-...
PGVECTOR_URL=postgresql://user:password@host:port/database
PGVECTOR_COLLECTION=meus_documentos
OPENAI_MODEL=text-embedding-3-small
```

### Vantagens do PGVector:
- **Busca sem√¢ntica**: Encontra documentos similares
- **Escalabilidade**: PostgreSQL √© robusto
- **Metadados**: Suporta informa√ß√µes adicionais
- **Performance**: √çndices otimizados para vetores

### Limita√ß√µes do PGVector:
- **Complexidade**: Requer PostgreSQL + pgvector
- **Configura√ß√£o**: Mais setup que solu√ß√µes simples
- **Custo**: Embeddings da OpenAI t√™m custo
- **Lat√™ncia**: Requisi√ß√µes para API de embeddings

### Exemplo de configura√ß√£o PostgreSQL:
```sql
-- Instalar extens√£o pgvector
CREATE EXTENSION IF NOT EXISTS vector;

-- Criar tabela para vetores
CREATE TABLE document_vectors (
    id TEXT PRIMARY KEY,
    embedding vector(1536),
    content TEXT,
    metadata JSONB
);
```

---

## üìö Script 4: 5-loaders-e-banco-de-dados-vetoriais/4-search-vector.py

### Explica√ß√£o do similarity_search_with_score():
```python
results = store.similarity_search_with_score(query, k=3)
```

**O que √© similarity_search_with_score:**
- **Busca documentos similares** usando similaridade vetorial
- **Retorna tuplas** (documento, score) ordenadas por similaridade
- **Score menor = mais similar** (dist√¢ncia vetorial)
- **Par√¢metro k**: N√∫mero de resultados a retornar

### Explica√ß√£o da query de busca:
```python
query = "Tell me more about the gpt-5 thinking evaluation and performance results comparing to gpt-4"
```

**O que √© a query:**
- **Texto de busca** que ser√° convertido em embedding
- **Pergunta ou termo** para encontrar documentos similares
- **Processo**: Query ‚Üí Embedding ‚Üí Compara√ß√£o com documentos no banco
- **Resultado**: Documentos mais semanticamente similares

### Explica√ß√£o do score de similaridade:
```python
for i, (doc, score) in enumerate(results, start=1):
    print(f"Resultado {i} (score: {score:.2f}):")
```

**Interpreta√ß√£o do score:**
- **0.0**: Documentos id√™nticos
- **0.0 - 0.3**: Muito similar (excelente match)
- **0.3 - 0.7**: Similaridade moderada (bom match)
- **0.7 - 1.0**: Baixa similaridade (match fraco)
- **> 1.0**: Muito diferente (match ruim)

### Fluxo completo de busca vetorial:

**1. Prepara√ß√£o da query:**
```python
query = "Tell me more about the gpt-5 thinking evaluation..."
# Query √© convertida em embedding usando o mesmo modelo da ingest√£o
```

**2. Busca no banco:**
```python
results = store.similarity_search_with_score(query, k=3)
# Retorna os 3 documentos mais similares com seus scores
```

**3. Processamento dos resultados:**
```python
for i, (doc, score) in enumerate(results, start=1):
    print(f"Resultado {i} (score: {score:.2f}):")
    print(doc.page_content)
    print(doc.metadata)
```

### Explica√ß√£o do par√¢metro k:
```python
results = store.similarity_search_with_score(query, k=3)
```

**O que √© k:**
- **N√∫mero de resultados** a retornar
- **k=3**: Retorna os 3 documentos mais similares
- **k=1**: Retorna apenas o mais similar
- **k=10**: Retorna os 10 mais similares
- **Escolha baseada** na aplica√ß√£o e performance

### Exibi√ß√£o estruturada dos resultados:
```python
for i, (doc, score) in enumerate(results, start=1):
    print("="*50)
    print(f"Resultado {i} (score: {score:.2f}):")
    print("="*50)
    
    print("\nTexto:\n")
    print(doc.page_content.strip())
    
    print("\nMetadados:\n")
    for k, v in doc.metadata.items():
        print(f"{k}: {v}")
```

**Estrutura da exibi√ß√£o:**
- **Separadores visuais**: `=` para delimitar cada resultado
- **Score formatado**: `{score:.2f}` para 2 casas decimais
- **Conte√∫do limpo**: `.strip()` remove espa√ßos extras
- **Metadados organizados**: Chave-valor formatados

### Vantagens da busca vetorial:
- **Busca sem√¢ntica**: Encontra documentos similares em significado
- **N√£o depende** de palavras-chave exatas
- **Escal√°vel**: Funciona com grandes volumes de documentos
- **Flex√≠vel**: Aceita queries em linguagem natural

### Limita√ß√µes da busca vetorial:
- **Custo**: Cada busca gera embedding (custo da OpenAI)
- **Lat√™ncia**: Requisi√ß√£o para API de embeddings
- **Qualidade**: Depende da qualidade dos embeddings
- **Contexto**: Pode n√£o capturar contexto espec√≠fico

### Exemplo de diferentes tipos de query:
```python
# Query espec√≠fica
query1 = "What are the performance benchmarks of GPT-5?"

# Query gen√©rica
query2 = "Tell me about AI models"

# Query com compara√ß√£o
query3 = "How does GPT-5 compare to GPT-4?"

# Query t√©cnica
query4 = "What are the technical specifications?"
```

### Exemplo de filtros por score:
```python
# Filtrar apenas resultados muito similares
good_results = [(doc, score) for doc, score in results if score < 0.3]

# Filtrar resultados moderadamente similares
moderate_results = [(doc, score) for doc, score in results if 0.3 <= score < 0.7]
```

### Compara√ß√£o com busca tradicional:

**Busca por palavras-chave:**
```python
# Busca exata (n√£o sem√¢ntica)
if "GPT-5" in document.content:
    return document
```
- ‚ùå N√£o encontra sin√¥nimos
- ‚ùå N√£o entende contexto
- ‚úÖ Muito r√°pida
- ‚úÖ Sem custo

**Busca vetorial:**
```python
# Busca sem√¢ntica
results = store.similarity_search_with_score(query, k=3)
```
- ‚úÖ Encontra documentos similares
- ‚úÖ Entende contexto e significado
- ‚ùå Mais lenta
- ‚ùå Tem custo

### Dicas para otimizar busca:
- **Queries espec√≠ficas**: Melhoram a precis√£o
- **Ajuste do k**: Balanceie quantidade e qualidade
- **Filtros por score**: Removem resultados irrelevantes
- **Metadados**: Use para filtrar por tipo de documento

---

## üìù Notas Gerais

### üîë Diferen√ßas entre m√©todos principais:

**ChatOpenAI vs init_chat_model:**
- **ChatOpenAI**: Espec√≠fico para OpenAI, mais direto e simples
- **init_chat_model**: Gen√©rico, funciona com m√∫ltiplos provedores (Google, OpenAI, Anthropic)

**PromptTemplate vs ChatPromptTemplate:**
- **PromptTemplate**: Templates simples de texto com vari√°veis
- **ChatPromptTemplate**: Templates estruturados com roles (system, human, assistant)

**@chain vs RunnableLambda:**
- **@chain**: Para fun√ß√µes que recebem e retornam dicion√°rios
- **RunnableLambda**: Para fun√ß√µes simples com input/output direto

### üéØ Dicas importantes para todos os scripts:
- **Sempre use `load_dotenv()`** para carregar vari√°veis de ambiente
- **Configure suas API keys** no arquivo `.env`
- **O m√©todo `invoke()`** √© usado para fazer chamadas aos modelos
- **Acesse a resposta** com `.content`
- **Use try/except** para tratar erros de API
- **Monitore custos** das APIs (especialmente OpenAI)

### üìä Resumo por Complexidade:

**üü¢ B√ÅSICO (Fundamentos):**
- Scripts 1-4 da Pasta 1: Conceitos fundamentais
- Script 1 da Pasta 2: LCEL b√°sico

**üü° INTERMEDI√ÅRIO (Chains e Mem√≥ria):**
- Scripts 2-7 da Pasta 2: Chains avan√ßadas
- Scripts 1-2 da Pasta 3: Agentes b√°sicos
- Scripts 1-2 da Pasta 4: Mem√≥ria de conversa

**üî¥ AVAN√áADO (Agentes e Vector DB):**
- Scripts 1-2 da Pasta 3: Agentes com tools
- Scripts 1-4 da Pasta 5: Loaders e banco vetorial

### üöÄ Pr√≥ximos Passos Sugeridos:

1. **Comece pelos fundamentos** (Pasta 1) - Entenda modelos e prompts
2. **Aprenda chains** (Pasta 2) - Domine o processamento sequencial
3. **Experimente agentes** (Pasta 3) - Automatize tarefas complexas
4. **Implemente mem√≥ria** (Pasta 4) - Adicione contexto √†s conversas
5. **Construa RAG** (Pasta 5) - Crie sistemas de busca em documentos

### üí° Projetos Pr√°ticos Sugeridos:

**üü¢ Iniciante:**
- Chatbot simples com mem√≥ria
- Sistema de tradu√ß√£o multi-idioma
- Gerador de resumos de textos

**üü° Intermedi√°rio:**
- Agente de pesquisa com m√∫ltiplas ferramentas
- Sistema de an√°lise de sentimentos
- Assistente de programa√ß√£o

**üî¥ Avan√ßado:**
- RAG completo com banco vetorial
- Agente multi-modal (texto + imagem)
- Sistema de recomenda√ß√£o baseado em embeddings

---

## üéì RESUMO EXECUTIVO

### üìà Progress√£o de Aprendizado

**FASE 1 - FUNDAMENTOS (Pasta 1)**
- ‚úÖ Entender modelos de linguagem (OpenAI, Google)
- ‚úÖ Dominar templates de prompt
- ‚úÖ Compreender diferen√ßas entre provedores

**FASE 2 - PROCESSAMENTO (Pasta 2)**
- ‚úÖ Criar chains sequenciais com LCEL
- ‚úÖ Implementar fun√ß√µes customizadas
- ‚úÖ Construir pipelines complexos
- ‚úÖ Dominar t√©cnicas de sumariza√ß√£o

**FASE 3 - AUTOMA√á√ÉO (Pasta 3)**
- ‚úÖ Criar agentes com ferramentas
- ‚úÖ Usar prompts da comunidade
- ‚úÖ Implementar racioc√≠nio e a√ß√£o

**FASE 4 - CONTEXTO (Pasta 4)**
- ‚úÖ Adicionar mem√≥ria √†s conversas
- ‚úÖ Controlar tamanho do hist√≥rico
- ‚úÖ Manter contexto entre intera√ß√µes

**FASE 5 - DADOS (Pasta 5)**
- ‚úÖ Carregar documentos de diferentes fontes
- ‚úÖ Criar e armazenar embeddings
- ‚úÖ Implementar busca sem√¢ntica
- ‚úÖ Construir sistemas RAG completos

### üèÜ Compet√™ncias Adquiridas

**T√©cnicas:**
- ‚úÖ Integra√ß√£o com m√∫ltiplos provedores de IA
- ‚úÖ Cria√ß√£o de prompts din√¢micos e estruturados
- ‚úÖ Constru√ß√£o de pipelines de processamento
- ‚úÖ Implementa√ß√£o de agentes aut√¥nomos
- ‚úÖ Gerenciamento de mem√≥ria conversacional
- ‚úÖ Cria√ß√£o de bancos de dados vetoriais
- ‚úÖ Implementa√ß√£o de busca sem√¢ntica

**Conceituais:**
- ‚úÖ Compreens√£o de LLMs e seus par√¢metros
- ‚úÖ Entendimento de embeddings e similaridade vetorial
- ‚úÖ Conhecimento de arquiteturas RAG
- ‚úÖ Familiaridade com agentes e ferramentas
- ‚úÖ Compreens√£o de processamento de linguagem natural

### üîÆ Aplica√ß√µes Pr√°ticas

**Desenvolvimento:**
- Chatbots inteligentes com mem√≥ria
- Sistemas de documenta√ß√£o automatizada
- Assistentes de programa√ß√£o
- An√°lise de dados com IA

**Neg√≥cios:**
- Atendimento ao cliente automatizado
- An√°lise de documentos e relat√≥rios
- Sistemas de recomenda√ß√£o
- Pesquisa e descoberta de informa√ß√µes

**Educa√ß√£o:**
- Tutores personalizados
- Sistemas de avalia√ß√£o autom√°tica
- Gera√ß√£o de conte√∫do educacional
- An√°lise de textos e reda√ß√µes

---

*Este arquivo ser√° atualizado conforme avan√ßarmos no estudo dos scripts.*
