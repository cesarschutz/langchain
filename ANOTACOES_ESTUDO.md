# üìö Anota√ß√µes de Estudo - LangChain

Este arquivo cont√©m anota√ß√µes e explica√ß√µes sobre cada script do projeto para facilitar o estudo e consulta.

---

# üåç PASTA 1: FUNDAMENTOS

---

## üåç Script 1: 1-fundamentos/1-hello-world.py

### Explica√ß√£o do ChatOpenAI:
```python
ChatOpenAI(model="gpt-5-nano", temperature=0.5)
```

**Par√¢metros:**
- **`model="gpt-5-nano"`**: Especifica qual modelo da OpenAI usar
  - `gpt-5-nano`: Modelo mais r√°pido e econ√¥mico
  - Outras op√ß√µes: `gpt-4`, `gpt-3.5-turbo`, `gpt-4-turbo`
- **`temperature=0.5`**: Controla a criatividade das respostas
  - `0.0`: Respostas muito determin√≠sticas e consistentes
  - `0.5`: Equil√≠brio entre criatividade e consist√™ncia
  - `1.0`: Respostas muito criativas e variadas

**Quando usar:**
- Para projetos espec√≠ficos da OpenAI
- Quando voc√™ precisa de controle fino sobre os par√¢metros
- Para integra√ß√µes diretas com modelos OpenAI

---

## üîß Script 2: 1-fundamentos/2-init-chat-model.py

### Explica√ß√£o do init_chat_model:
```python
gemini = init_chat_model(model="gemini-2.5-flash", model_provider="google_genai")
```

**Par√¢metros:**
- **`model="gemini-2.5-flash"`**: Especifica o modelo espec√≠fico do provedor
  - `gemini-2.5-flash`: Modelo r√°pido do Google
  - Outras op√ß√µes: `gemini-2.0-flash`, `gemini-1.5-pro`
- **`model_provider="google_genai"`**: Indica qual provedor usar
  - `google_genai`: Para modelos Google Gemini
  - `openai`: Para modelos OpenAI
  - `anthropic`: Para modelos Anthropic Claude

**Vantagens:**
- Flexibilidade para trocar entre provedores
- C√≥digo mais gen√©rico e reutiliz√°vel
- Suporte a m√∫ltiplos provedores de IA

**Quando usar:**
- Para projetos que podem usar diferentes provedores
- Quando voc√™ quer flexibilidade para trocar modelos
- Para c√≥digo mais port√°vel entre diferentes servi√ßos

---

## üìù Script 3: 1-fundamentos/3-prompt-template.py

### Explica√ß√£o do PromptTemplate:
```python
template = PromptTemplate(
    input_variables=["name"],
    template="Hi, I'm {name}! Tell me a joke with my name!"
)
```

**Par√¢metros:**
- **`input_variables=["name"]`**: Lista das vari√°veis que o template aceita
  - Define quais placeholders podem ser usados no template
  - Pode ter m√∫ltiplas vari√°veis: `["name", "age", "city"]`
- **`template="..."`**: O texto do prompt com placeholders
  - Use `{variavel}` para criar placeholders
  - O texto pode ser qualquer prompt que voc√™ queira reutilizar

### Explica√ß√£o do m√©todo format():
```python
text = template.format(name="Wesley")
```

**Funcionalidade:**
- **Substitui as vari√°veis** por valores reais
- **`name="Wesley"`**: Substitui `{name}` por "Wesley" no template
- **Retorna o prompt final** pronto para ser usado

### Vantagens dos Templates:
- **Reutiliza√ß√£o**: Crie um template e use com diferentes valores
- **Consist√™ncia**: Mant√©m a estrutura do prompt sempre igual
- **Manutenibilidade**: Mude o template uma vez, afeta todos os usos
- **Legibilidade**: C√≥digo mais limpo e organizado

### Exemplo pr√°tico:
```python
# Template criado uma vez
template = PromptTemplate(
    input_variables=["name", "age"],
    template="Ol√° {name}, voc√™ tem {age} anos!"
)

# Usado m√∫ltiplas vezes
text1 = template.format(name="Jo√£o", age="25")
text2 = template.format(name="Maria", age="30")
# Resultado: "Ol√° Jo√£o, voc√™ tem 25 anos!" e "Ol√° Maria, voc√™ tem 30 anos!"
```

---

## üí¨ Script 4: 1-fundamentos/4-chat-prompt-template.py

### Explica√ß√£o do ChatPromptTemplate:
```python
system = ("system", "you are an assistant that answers questions in a {style} style")
user = ("user", "{question}")

chat_prompt = ChatPromptTemplate([system, user])
```

**Diferen√ßa do PromptTemplate simples:**
- **PromptTemplate**: Cria um texto √∫nico com vari√°veis
- **ChatPromptTemplate**: Cria m√∫ltiplas mensagens estruturadas (system, user, assistant)

**Estrutura das mensagens:**
- **`("system", "...")`**: Define o papel/comportamento da IA
  - Controla como a IA deve se comportar
  - Pode definir estilo, tom, especializa√ß√£o
- **`("user", "...")`**: Define a entrada/pergunta do usu√°rio
  - O que o usu√°rio est√° perguntando
  - Pode conter vari√°veis para personaliza√ß√£o

### Explica√ß√£o do format_messages():
```python
messages = chat_prompt.format_messages(style="funny", question="Who is Alan Turing?")
```

**Funcionalidade:**
- **Substitui vari√°veis** em todas as mensagens do template
- **Retorna lista de mensagens** estruturadas
- **Preserva a estrutura** system/user/assistant

### Vantagens do ChatPromptTemplate:
- **Estrutura de conversa**: Mant√©m o contexto de chat
- **Controle de comportamento**: System message define o papel da IA
- **Flexibilidade**: Pode ter m√∫ltiplas mensagens (system, user, assistant)
- **Compatibilidade**: Funciona melhor com modelos de chat

### Exemplo de sa√≠da:
```python
# Ap√≥s format_messages():
# system: you are an assistant that answers questions in a funny style
# user: Who is Alan Turing?

# Ap√≥s invoke():
# assistant: [Resposta engra√ßada sobre Alan Turing]
```

### Compara√ß√£o com PromptTemplate:
```python
# PromptTemplate (simples):
template = PromptTemplate(
    input_variables=["name"],
    template="Hi, I'm {name}!"
)

# ChatPromptTemplate (estruturado):
chat_prompt = ChatPromptTemplate([
    ("system", "You are a helpful assistant"),
    ("user", "Tell me about {topic}")
])
```

---

# üîó PASTA 2: CHAINS E PROCESSAMENTO

---

## üîó Script 1: 2-chains-e-processamento/1-iniciando-com-chains.py

### Explica√ß√£o do operador pipe (|):
```python
chain = question_template | model
```

**O que √© o operador |:**
- **LCEL (LangChain Expression Language)**: Sintaxe para conectar componentes
- **Operador pipe (|)**: Conecta componentes em sequ√™ncia
- **Pipeline**: Cria um fluxo de processamento: template ‚Üí modelo

**Como funciona:**
1. **question_template**: Formata o prompt com as vari√°veis
2. **|**: Conecta o template formatado ao modelo
3. **model**: Recebe o prompt formatado e gera a resposta

### Explica√ß√£o do invoke com dicion√°rio:
```python
result = chain.invoke({"name": "Wesley"})
```

**Diferen√ßa do invoke simples:**
- **Modelo simples**: `model.invoke("texto direto")`
- **Chain**: `chain.invoke({"variavel": "valor"})`

**Vantagens das Chains:**
- **Modularidade**: Cada componente tem uma fun√ß√£o espec√≠fica
- **Reutiliza√ß√£o**: Pode trocar componentes facilmente
- **Legibilidade**: C√≥digo mais limpo e organizado
- **Flexibilidade**: Pode adicionar mais componentes na sequ√™ncia

### Exemplo de fluxo:
```python
# 1. Template formata: "Ola, eu sou o Wesley! conteme uma piada!"
# 2. Modelo recebe o texto formatado
# 3. Modelo gera a resposta
# 4. Resultado final √© retornado
```

---

## üîß Script 2: 2-chains-e-processamento/2-chains-com-decorators.py

### Explica√ß√£o do decorador @chain:
```python
@chain
def square(input_dict:dict) -> dict:
    x = input_dict["x"]
    return {"square_result": x * x}
```

**O que √© o decorador @chain:**
- **Transforma fun√ß√µes Python** em componentes compat√≠veis com LCEL
- **Permite criar l√≥gica customizada** entre componentes do LangChain
- **Mant√©m a compatibilidade** com o operador pipe (|)

**Estrutura da fun√ß√£o:**
- **`input_dict:dict`**: Recebe um dicion√°rio como entrada
- **`-> dict`**: Retorna um dicion√°rio como sa√≠da
- **Deve seguir o padr√£o**: entrada e sa√≠da como dicion√°rios

### Explica√ß√£o da chain com 3 componentes:
```python
chain2 = square | question_template2 | model
```

**Fluxo de execu√ß√£o:**
1. **square**: Recebe `{"x": 10}` ‚Üí retorna `{"square_result": 100}`
2. **question_template2**: Recebe `{"square_result": 100}` ‚Üí formata prompt
3. **model**: Recebe o prompt formatado ‚Üí gera resposta

### Vantagens das fun√ß√µes personalizadas:
- **L√≥gica customizada**: Pode adicionar qualquer processamento
- **Reutiliza√ß√£o**: Fun√ß√µes podem ser usadas em diferentes chains
- **Flexibilidade**: Pode conectar l√≥gica Python pura com LangChain
- **Testabilidade**: Fun√ß√µes podem ser testadas independentemente

### Exemplo de uso:
```python
# Fun√ß√£o personalizada
@chain
def process_data(input_dict: dict) -> dict:
    data = input_dict["data"]
    processed = data.upper()  # Qualquer l√≥gica Python
    return {"processed_data": processed}

# Chain com fun√ß√£o customizada
chain = process_data | template | model
result = chain.invoke({"data": "hello world"})
```

---

## ‚ö° Script 3: 2-chains-e-processamento/3-runnable-lambda.py

### Explica√ß√£o do RunnableLambda:
```python
def parse_number(text:str) -> int:
    return int(text.strip())

parse_runnable = RunnableLambda(parse_number)
```

**O que √© RunnableLambda:**
- **Transforma fun√ß√µes Python simples** em componentes LCEL
- **Alternativa mais simples** ao decorador @chain
- **Permite tipos diretos** (n√£o precisa de dicion√°rios)

### Diferen√ßa entre @chain e RunnableLambda:

**@chain (decorador):**
```python
@chain
def square(input_dict:dict) -> dict:
    x = input_dict["x"]
    return {"square_result": x * x}  # SEMPRE dicion√°rio
```

**RunnableLambda:**
```python
def parse_number(text:str) -> int:
    return int(text.strip())  # Pode retornar qualquer tipo

parse_runnable = RunnableLambda(parse_number)
```

### Quando usar cada um:

**Use @chain quando:**
- ‚úÖ Integrando com outros componentes LangChain
- ‚úÖ Precisando passar dados entre componentes
- ‚úÖ Trabalhando em pipelines complexos

**Use RunnableLambda quando:**
- ‚úÖ Fun√ß√£o simples com tipos diretos
- ‚úÖ Processamento b√°sico de dados
- ‚úÖ N√£o precisa de entrada/sa√≠da como dicion√°rio

### Vantagens do RunnableLambda:
- **Simplicidade**: N√£o precisa de dicion√°rios
- **Flexibilidade**: Pode usar qualquer tipo de entrada/sa√≠da
- **Legibilidade**: C√≥digo mais direto e simples
- **Reutiliza√ß√£o**: Fun√ß√µes Python existentes podem ser facilmente convertidas

### Exemplo de uso:
```python
# Fun√ß√£o simples
def clean_text(text: str) -> str:
    return text.strip().lower()

# Transforma em componente
clean_runnable = RunnableLambda(clean_text)

# Usa em chain
chain = clean_runnable | template | model
result = chain.invoke("  HELLO WORLD  ")
```

---

## üîÑ Script 4: 2-chains-e-processamento/4-pipeline-de-processamento.py

### Explica√ß√£o do StrOutputParser:
```python
from langchain_core.output_parsers import StrOutputParser

translate = template_translate | llm_en | StrOutputParser()
```

**O que √© StrOutputParser:**
- **Extrai apenas o texto** das respostas dos modelos
- **Remove metadados** e informa√ß√µes extras
- **Retorna string pura** em vez de objeto complexo

### Explica√ß√£o do pipeline complexo:
```python
pipeline = {"text": translate} | template_summary | llm_en | StrOutputParser()
```

**Estrutura do pipeline:**
1. **`{"text": translate}`**: Mapeia a sa√≠da da tradu√ß√£o para a vari√°vel "text"
2. **`template_summary`**: Usa o texto traduzido para criar prompt de sumariza√ß√£o
3. **`llm_en`**: Gera a sumariza√ß√£o
4. **`StrOutputParser()`**: Extrai apenas o texto da resposta

### Fluxo de execu√ß√£o:
```python
# 1. Entrada: "LangChain √© um framework para desenvolvimento de aplica√ß√µes de IA"
# 2. Tradu√ß√£o: "LangChain is a framework for AI application development"
# 3. Sumariza√ß√£o: "AI framework for development"
# 4. Sa√≠da: "AI framework for development"
```

### Vantagens dos pipelines complexos:
- **Processamento sequencial**: M√∫ltiplas etapas em uma √∫nica chain
- **Reutiliza√ß√£o**: Cada etapa pode ser usada independentemente
- **Modularidade**: F√°cil de modificar ou adicionar etapas
- **Efici√™ncia**: Processamento otimizado em sequ√™ncia

### Exemplo de uso do StrOutputParser:
```python
# Sem StrOutputParser:
response = model.invoke("Hello")
# Resultado: AIMessage(content="Hello there!", additional_kwargs={}, ...)

# Com StrOutputParser:
chain = model | StrOutputParser()
response = chain.invoke("Hello")
# Resultado: "Hello there!"
```

### Padr√£o de mapeamento com dicion√°rio:
```python
# Mapeia sa√≠da de uma chain para entrada de outra
pipeline = {"variavel": chain_anterior} | proxima_chain

# Exemplo:
chain1 = template1 | model1 | StrOutputParser()
chain2 = template2 | model2 | StrOutputParser()
pipeline = {"texto_processado": chain1} | chain2
```

---

## üìù Script 5: 2-chains-e-processamento/5-sumarizacao.py

### Explica√ß√£o do RecursiveCharacterTextSplitter:
```python
splitter = RecursiveCharacterTextSplitter(
    chunk_size=250, chunk_overlap=70, 
)
parts = splitter.create_documents([long_text])
```

**O que √© RecursiveCharacterTextSplitter:**
- **Divide textos longos** em chunks menores
- **Mant√©m contexto** entre chunks com sobreposi√ß√£o
- **Permite processamento** de textos que excedem o limite dos modelos

**Par√¢metros importantes:**
- **`chunk_size=250`**: Tamanho m√°ximo de cada chunk (caracteres)
- **`chunk_overlap=70`**: Sobreposi√ß√£o entre chunks para manter contexto
- **`create_documents()`**: Converte texto em lista de documentos

### Explica√ß√£o da t√©cnica "stuff":
```python
chain_sumarize = load_summarize_chain(llm, chain_type="stuff", verbose=False)
```

**O que √© a t√©cnica "stuff":**
- **Coloca todo o texto** em um √∫nico prompt
- **Funciona bem** para textos que cabem no contexto do modelo
- **Processamento simples** e direto

**Vantagens:**
- ‚úÖ Simples de implementar
- ‚úÖ Mant√©m contexto completo
- ‚úÖ R√°pido para textos menores

**Limita√ß√µes:**
- ‚ùå Limitado pelo tamanho do contexto do modelo
- ‚ùå Pode ser ineficiente para textos muito longos

### Fluxo de processamento:
```python
# 1. Texto longo ‚Üí RecursiveCharacterTextSplitter
# 2. Chunks menores ‚Üí load_summarize_chain
# 3. T√©cnica "stuff" ‚Üí Resumo final
```

### Exemplo de uso:
```python
# Dividir texto
splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=70)
parts = splitter.create_documents([texto_longo])

# Sumarizar
chain = load_summarize_chain(llm, chain_type="stuff")
result = chain.invoke({"input_documents": parts})
resumo = result["output_text"]
```

### Quando usar a t√©cnica "stuff":
- ‚úÖ Textos que cabem no contexto do modelo
- ‚úÖ Sumariza√ß√µes simples e diretas
- ‚úÖ Quando voc√™ quer manter todo o contexto
- ‚úÖ Processamento r√°pido

---

## üîÑ Script 6: 2-chains-e-processamento/6-sumarizacao-com-map-reduce.py

### Explica√ß√£o da t√©cnica "map_reduce":
```python
chain_sumarize = load_summarize_chain(llm, chain_type="map_reduce", verbose=False)
```

**O que √© a t√©cnica "map_reduce":**
- **Processa cada chunk separadamente** (fase "map")
- **Combina os resumos** em um resumo final (fase "reduce")
- **Funciona com textos muito longos** que n√£o cabem no contexto

### Diferen√ßa detalhada entre "stuff" e "map_reduce":

**T√©cnica "stuff" (Script 5):**
```python
chain_type="stuff"
```
- **Processamento**: Coloca TODO o texto em um √∫nico prompt
- **Limita√ß√£o**: Limitado pelo tamanho do contexto do modelo
- **Velocidade**: Mais r√°pido (uma √∫nica chamada)
- **Contexto**: Mant√©m contexto completo
- **Uso**: Textos que cabem no contexto

**T√©cnica "map_reduce" (Script 6):**
```python
chain_type="map_reduce"
```
- **Processamento**: 
  1. **Map**: Sumariza cada chunk separadamente
  2. **Reduce**: Combina todos os resumos em um resumo final
- **Limita√ß√£o**: N√£o tem limite de tamanho (processa por partes)
- **Velocidade**: Mais lento (m√∫ltiplas chamadas)
- **Contexto**: Pode perder contexto entre chunks
- **Uso**: Textos muito longos

### Fluxo de processamento map_reduce:
```python
# Fase 1 - MAP: Processa cada chunk
Chunk 1 ‚Üí Sumariza√ß√£o ‚Üí Resumo 1
Chunk 2 ‚Üí Sumariza√ß√£o ‚Üí Resumo 2
Chunk 3 ‚Üí Sumariza√ß√£o ‚Üí Resumo 3
...

# Fase 2 - REDUCE: Combina os resumos
[Resumo 1, Resumo 2, Resumo 3, ...] ‚Üí Sumariza√ß√£o Final ‚Üí Resumo Final
```

### Compara√ß√£o pr√°tica:

**"Stuff" (Script 5):**
```python
# Uma √∫nica chamada
Texto completo ‚Üí Modelo ‚Üí Resumo
```

**"Map_reduce" (Script 6):**
```python
# M√∫ltiplas chamadas
Chunk 1 ‚Üí Modelo ‚Üí Resumo 1
Chunk 2 ‚Üí Modelo ‚Üí Resumo 2
...
[Resumos] ‚Üí Modelo ‚Üí Resumo Final
```

### Quando usar cada t√©cnica:

**Use "stuff" quando:**
- ‚úÖ Texto cabe no contexto do modelo
- ‚úÖ Quer processamento r√°pido
- ‚úÖ Precisa manter contexto completo
- ‚úÖ Textos at√© ~4000 tokens

**Use "map_reduce" quando:**
- ‚úÖ Texto √© muito longo (excede contexto)
- ‚úÖ Pode aceitar perda de contexto entre partes
- ‚úÖ Precisa processar documentos grandes
- ‚úÖ Textos com mais de ~4000 tokens

### Vantagens e desvantagens:

**"Stuff":**
- ‚úÖ R√°pido, mant√©m contexto, simples
- ‚ùå Limitado pelo contexto do modelo

**"Map_reduce":**
- ‚úÖ Funciona com textos ilimitados, escal√°vel
- ‚ùå Mais lento, pode perder contexto, complexo

---

## üîß Script 7: 2-chains-e-processamento/7-pipeline-de-sumarizacao.py

### Explica√ß√£o do pipeline manual map-reduce:
```python
# Fase MAP
map_prompt = PromptTemplate.from_template("Write a concise summary of the following text:\n{context}")
map_chain = map_prompt | llm | StrOutputParser()
prepare_map_inputs = RunnableLambda(lambda docs: [{"context": d.page_content} for d in docs])
map_stage = prepare_map_inputs | map_chain.map()

# Fase REDUCE
reduce_prompt = PromptTemplate.from_template("Combine the following summaries into a single concise summary:\n{context}")
reduce_chain = reduce_prompt | llm | StrOutputParser()
prepare_reduce_input = RunnableLambda(lambda summaries: {"context": "\n".join(summaries)})

# Pipeline completo
pipeline = map_stage | prepare_reduce_input | reduce_chain
```

### Diferen√ßa entre pipeline manual e load_summarize_chain:

**load_summarize_chain (Script 6):**
```python
chain = load_summarize_chain(llm, chain_type="map_reduce")
```
- ‚úÖ Mais simples de implementar
- ‚ùå Menos controle sobre cada etapa
- ‚ùå Prompts fixos (n√£o personaliz√°veis)

**Pipeline manual (Script 7):**
```python
# Controle total sobre cada etapa
map_prompt = PromptTemplate.from_template("Seu prompt personalizado")
reduce_prompt = PromptTemplate.from_template("Seu prompt personalizado")
```
- ‚úÖ Controle total sobre prompts
- ‚úÖ Personaliza√ß√£o completa
- ‚úÖ Flexibilidade m√°xima
- ‚ùå Mais c√≥digo para implementar

### Explica√ß√£o do PromptTemplate.from_template():
```python
map_prompt = PromptTemplate.from_template("Write a concise summary of the following text:\n{context}")
```

**Vantagem:**
- **Cria√ß√£o r√°pida** de templates simples
- **N√£o precisa definir** input_variables explicitamente
- **Detecta automaticamente** as vari√°veis no template

### Explica√ß√£o do m√©todo .map():
```python
map_stage = prepare_map_inputs | map_chain.map()
```

**O que faz:**
- **Aplica a chain** a cada item da lista de inputs
- **Processamento paralelo** de m√∫ltiplos documentos
- **Retorna lista** de resultados

### Fluxo detalhado do pipeline:

**Fase MAP:**
```python
# 1. prepare_map_inputs: Converte documentos em lista de dicion√°rios
# 2. map_chain.map(): Aplica sumariza√ß√£o a cada chunk
# 3. Resultado: Lista de resumos individuais
```

**Fase REDUCE:**
```python
# 1. prepare_reduce_input: Junta todos os resumos em uma string
# 2. reduce_chain: Combina os resumos em um resumo final
# 3. Resultado: Resumo final consolidado
```

### Vantagens do pipeline manual:
- **Controle total**: Personaliza cada etapa
- **Prompts customizados**: Adapta para seu caso de uso
- **Flexibilidade**: Pode adicionar etapas intermedi√°rias
- **Debugging**: F√°cil de identificar problemas
- **Otimiza√ß√£o**: Pode ajustar cada componente

### Exemplo de personaliza√ß√£o:
```python
# Prompt personalizado para sumariza√ß√£o
map_prompt = PromptTemplate.from_template(
    "Summarize this text in exactly 3 bullet points:\n{context}"
)

# Prompt personalizado para combina√ß√£o
reduce_prompt = PromptTemplate.from_template(
    "Create a poetic summary of these summaries:\n{context}"
)
```

---

## üìù Notas Gerais

### Diferen√ßa entre os m√©todos:
- **ChatOpenAI**: Espec√≠fico para OpenAI, mais direto
- **init_chat_model**: Gen√©rico, funciona com m√∫ltiplos provedores

### Dicas importantes:
- Sempre use `load_dotenv()` para carregar vari√°veis de ambiente
- Configure suas API keys no arquivo `.env`
- O m√©todo `invoke()` √© usado para fazer chamadas aos modelos
- Acesse a resposta com `.content`

---

*Este arquivo ser√° atualizado conforme avan√ßarmos no estudo dos scripts.*
