# üìö Anota√ß√µes de Estudo - LangChain

Este arquivo cont√©m anota√ß√µes e explica√ß√µes sobre cada script do projeto para facilitar o estudo e consulta.

---

# üåç PASTA 1: FUNDAMENTOS

---

## üåç Script 1: 1-fundamentos/1-hello-world.py

### Explica√ß√£o do ChatOpenAI:
```python
ChatOpenAI(model="gpt-5-nano", temperature=0.5)
```

**Par√¢metros:**
- **`model="gpt-5-nano"`**: Especifica qual modelo da OpenAI usar
  - `gpt-5-nano`: Modelo mais r√°pido e econ√¥mico
  - Outras op√ß√µes: `gpt-4`, `gpt-3.5-turbo`, `gpt-4-turbo`
- **`temperature=0.5`**: Controla a criatividade das respostas
  - `0.0`: Respostas muito determin√≠sticas e consistentes
  - `0.5`: Equil√≠brio entre criatividade e consist√™ncia
  - `1.0`: Respostas muito criativas e variadas

**Quando usar:**
- Para projetos espec√≠ficos da OpenAI
- Quando voc√™ precisa de controle fino sobre os par√¢metros
- Para integra√ß√µes diretas com modelos OpenAI

---

## üîß Script 2: 1-fundamentos/2-init-chat-model.py

### Explica√ß√£o do init_chat_model:
```python
gemini = init_chat_model(model="gemini-2.5-flash", model_provider="google_genai")
```

**Par√¢metros:**
- **`model="gemini-2.5-flash"`**: Especifica o modelo espec√≠fico do provedor
  - `gemini-2.5-flash`: Modelo r√°pido do Google
  - Outras op√ß√µes: `gemini-2.0-flash`, `gemini-1.5-pro`
- **`model_provider="google_genai"`**: Indica qual provedor usar
  - `google_genai`: Para modelos Google Gemini
  - `openai`: Para modelos OpenAI
  - `anthropic`: Para modelos Anthropic Claude

**Vantagens:**
- Flexibilidade para trocar entre provedores
- C√≥digo mais gen√©rico e reutiliz√°vel
- Suporte a m√∫ltiplos provedores de IA

**Quando usar:**
- Para projetos que podem usar diferentes provedores
- Quando voc√™ quer flexibilidade para trocar modelos
- Para c√≥digo mais port√°vel entre diferentes servi√ßos

---

## üìù Script 3: 1-fundamentos/3-prompt-template.py

### Explica√ß√£o do PromptTemplate:
```python
template = PromptTemplate(
    input_variables=["name"],
    template="Hi, I'm {name}! Tell me a joke with my name!"
)
```

**Par√¢metros:**
- **`input_variables=["name"]`**: Lista das vari√°veis que o template aceita
  - Define quais placeholders podem ser usados no template
  - Pode ter m√∫ltiplas vari√°veis: `["name", "age", "city"]`
- **`template="..."`**: O texto do prompt com placeholders
  - Use `{variavel}` para criar placeholders
  - O texto pode ser qualquer prompt que voc√™ queira reutilizar

### Explica√ß√£o do m√©todo format():
```python
text = template.format(name="Wesley")
```

**Funcionalidade:**
- **Substitui as vari√°veis** por valores reais
- **`name="Wesley"`**: Substitui `{name}` por "Wesley" no template
- **Retorna o prompt final** pronto para ser usado

### Vantagens dos Templates:
- **Reutiliza√ß√£o**: Crie um template e use com diferentes valores
- **Consist√™ncia**: Mant√©m a estrutura do prompt sempre igual
- **Manutenibilidade**: Mude o template uma vez, afeta todos os usos
- **Legibilidade**: C√≥digo mais limpo e organizado

### Exemplo pr√°tico:
```python
# Template criado uma vez
template = PromptTemplate(
    input_variables=["name", "age"],
    template="Ol√° {name}, voc√™ tem {age} anos!"
)

# Usado m√∫ltiplas vezes
text1 = template.format(name="Jo√£o", age="25")
text2 = template.format(name="Maria", age="30")
# Resultado: "Ol√° Jo√£o, voc√™ tem 25 anos!" e "Ol√° Maria, voc√™ tem 30 anos!"
```

---

## üí¨ Script 4: 1-fundamentos/4-chat-prompt-template.py

### Explica√ß√£o do ChatPromptTemplate:
```python
system = ("system", "you are an assistant that answers questions in a {style} style")
user = ("user", "{question}")

chat_prompt = ChatPromptTemplate([system, user])
```

**Diferen√ßa do PromptTemplate simples:**
- **PromptTemplate**: Cria um texto √∫nico com vari√°veis
- **ChatPromptTemplate**: Cria m√∫ltiplas mensagens estruturadas (system, user, assistant)

**Estrutura das mensagens:**
- **`("system", "...")`**: Define o papel/comportamento da IA
  - Controla como a IA deve se comportar
  - Pode definir estilo, tom, especializa√ß√£o
- **`("user", "...")`**: Define a entrada/pergunta do usu√°rio
  - O que o usu√°rio est√° perguntando
  - Pode conter vari√°veis para personaliza√ß√£o

### Explica√ß√£o do format_messages():
```python
messages = chat_prompt.format_messages(style="funny", question="Who is Alan Turing?")
```

**Funcionalidade:**
- **Substitui vari√°veis** em todas as mensagens do template
- **Retorna lista de mensagens** estruturadas
- **Preserva a estrutura** system/user/assistant

### Vantagens do ChatPromptTemplate:
- **Estrutura de conversa**: Mant√©m o contexto de chat
- **Controle de comportamento**: System message define o papel da IA
- **Flexibilidade**: Pode ter m√∫ltiplas mensagens (system, user, assistant)
- **Compatibilidade**: Funciona melhor com modelos de chat

### Exemplo de sa√≠da:
```python
# Ap√≥s format_messages():
# system: you are an assistant that answers questions in a funny style
# user: Who is Alan Turing?

# Ap√≥s invoke():
# assistant: [Resposta engra√ßada sobre Alan Turing]
```

### Compara√ß√£o com PromptTemplate:
```python
# PromptTemplate (simples):
template = PromptTemplate(
    input_variables=["name"],
    template="Hi, I'm {name}!"
)

# ChatPromptTemplate (estruturado):
chat_prompt = ChatPromptTemplate([
    ("system", "You are a helpful assistant"),
    ("user", "Tell me about {topic}")
])
```

---

# üîó PASTA 2: CHAINS E PROCESSAMENTO

---

## üîó Script 1: 2-chains-e-processamento/1-iniciando-com-chains.py

### Explica√ß√£o do operador pipe (|):
```python
chain = question_template | model
```

**O que √© o operador |:**
- **LCEL (LangChain Expression Language)**: Sintaxe para conectar componentes
- **Operador pipe (|)**: Conecta componentes em sequ√™ncia
- **Pipeline**: Cria um fluxo de processamento: template ‚Üí modelo

**Como funciona:**
1. **question_template**: Formata o prompt com as vari√°veis
2. **|**: Conecta o template formatado ao modelo
3. **model**: Recebe o prompt formatado e gera a resposta

### Explica√ß√£o do invoke com dicion√°rio:
```python
result = chain.invoke({"name": "Wesley"})
```

**Diferen√ßa do invoke simples:**
- **Modelo simples**: `model.invoke("texto direto")`
- **Chain**: `chain.invoke({"variavel": "valor"})`

**Vantagens das Chains:**
- **Modularidade**: Cada componente tem uma fun√ß√£o espec√≠fica
- **Reutiliza√ß√£o**: Pode trocar componentes facilmente
- **Legibilidade**: C√≥digo mais limpo e organizado
- **Flexibilidade**: Pode adicionar mais componentes na sequ√™ncia

### Exemplo de fluxo:
```python
# 1. Template formata: "Ola, eu sou o Wesley! conteme uma piada!"
# 2. Modelo recebe o texto formatado
# 3. Modelo gera a resposta
# 4. Resultado final √© retornado
```

---

## üîß Script 2: 2-chains-e-processamento/2-chains-com-decorators.py

### Explica√ß√£o do decorador @chain:
```python
@chain
def square(input_dict:dict) -> dict:
    x = input_dict["x"]
    return {"square_result": x * x}
```

**O que √© o decorador @chain:**
- **Transforma fun√ß√µes Python** em componentes compat√≠veis com LCEL
- **Permite criar l√≥gica customizada** entre componentes do LangChain
- **Mant√©m a compatibilidade** com o operador pipe (|)

**Estrutura da fun√ß√£o:**
- **`input_dict:dict`**: Recebe um dicion√°rio como entrada
- **`-> dict`**: Retorna um dicion√°rio como sa√≠da
- **Deve seguir o padr√£o**: entrada e sa√≠da como dicion√°rios

### Explica√ß√£o da chain com 3 componentes:
```python
chain2 = square | question_template2 | model
```

**Fluxo de execu√ß√£o:**
1. **square**: Recebe `{"x": 10}` ‚Üí retorna `{"square_result": 100}`
2. **question_template2**: Recebe `{"square_result": 100}` ‚Üí formata prompt
3. **model**: Recebe o prompt formatado ‚Üí gera resposta

### Vantagens das fun√ß√µes personalizadas:
- **L√≥gica customizada**: Pode adicionar qualquer processamento
- **Reutiliza√ß√£o**: Fun√ß√µes podem ser usadas em diferentes chains
- **Flexibilidade**: Pode conectar l√≥gica Python pura com LangChain
- **Testabilidade**: Fun√ß√µes podem ser testadas independentemente

### Exemplo de uso:
```python
# Fun√ß√£o personalizada
@chain
def process_data(input_dict: dict) -> dict:
    data = input_dict["data"]
    processed = data.upper()  # Qualquer l√≥gica Python
    return {"processed_data": processed}

# Chain com fun√ß√£o customizada
chain = process_data | template | model
result = chain.invoke({"data": "hello world"})
```

---

## ‚ö° Script 3: 2-chains-e-processamento/3-runnable-lambda.py

### Explica√ß√£o do RunnableLambda:
```python
def parse_number(text:str) -> int:
    return int(text.strip())

parse_runnable = RunnableLambda(parse_number)
```

**O que √© RunnableLambda:**
- **Transforma fun√ß√µes Python simples** em componentes LCEL
- **Alternativa mais simples** ao decorador @chain
- **Permite tipos diretos** (n√£o precisa de dicion√°rios)

### Diferen√ßa entre @chain e RunnableLambda:

**@chain (decorador):**
```python
@chain
def square(input_dict:dict) -> dict:
    x = input_dict["x"]
    return {"square_result": x * x}  # SEMPRE dicion√°rio
```

**RunnableLambda:**
```python
def parse_number(text:str) -> int:
    return int(text.strip())  # Pode retornar qualquer tipo

parse_runnable = RunnableLambda(parse_number)
```

### Quando usar cada um:

**Use @chain quando:**
- ‚úÖ Integrando com outros componentes LangChain
- ‚úÖ Precisando passar dados entre componentes
- ‚úÖ Trabalhando em pipelines complexos

**Use RunnableLambda quando:**
- ‚úÖ Fun√ß√£o simples com tipos diretos
- ‚úÖ Processamento b√°sico de dados
- ‚úÖ N√£o precisa de entrada/sa√≠da como dicion√°rio

### Vantagens do RunnableLambda:
- **Simplicidade**: N√£o precisa de dicion√°rios
- **Flexibilidade**: Pode usar qualquer tipo de entrada/sa√≠da
- **Legibilidade**: C√≥digo mais direto e simples
- **Reutiliza√ß√£o**: Fun√ß√µes Python existentes podem ser facilmente convertidas

### Exemplo de uso:
```python
# Fun√ß√£o simples
def clean_text(text: str) -> str:
    return text.strip().lower()

# Transforma em componente
clean_runnable = RunnableLambda(clean_text)

# Usa em chain
chain = clean_runnable | template | model
result = chain.invoke("  HELLO WORLD  ")
```

---

## üîÑ Script 4: 2-chains-e-processamento/4-pipeline-de-processamento.py

### Explica√ß√£o do StrOutputParser:
```python
from langchain_core.output_parsers import StrOutputParser

translate = template_translate | llm_en | StrOutputParser()
```

**O que √© StrOutputParser:**
- **Extrai apenas o texto** das respostas dos modelos
- **Remove metadados** e informa√ß√µes extras
- **Retorna string pura** em vez de objeto complexo

### Explica√ß√£o do pipeline complexo:
```python
pipeline = {"text": translate} | template_summary | llm_en | StrOutputParser()
```

**Estrutura do pipeline:**
1. **`{"text": translate}`**: Mapeia a sa√≠da da tradu√ß√£o para a vari√°vel "text"
2. **`template_summary`**: Usa o texto traduzido para criar prompt de sumariza√ß√£o
3. **`llm_en`**: Gera a sumariza√ß√£o
4. **`StrOutputParser()`**: Extrai apenas o texto da resposta

### Fluxo de execu√ß√£o:
```python
# 1. Entrada: "LangChain √© um framework para desenvolvimento de aplica√ß√µes de IA"
# 2. Tradu√ß√£o: "LangChain is a framework for AI application development"
# 3. Sumariza√ß√£o: "AI framework for development"
# 4. Sa√≠da: "AI framework for development"
```

### Vantagens dos pipelines complexos:
- **Processamento sequencial**: M√∫ltiplas etapas em uma √∫nica chain
- **Reutiliza√ß√£o**: Cada etapa pode ser usada independentemente
- **Modularidade**: F√°cil de modificar ou adicionar etapas
- **Efici√™ncia**: Processamento otimizado em sequ√™ncia

### Exemplo de uso do StrOutputParser:
```python
# Sem StrOutputParser:
response = model.invoke("Hello")
# Resultado: AIMessage(content="Hello there!", additional_kwargs={}, ...)

# Com StrOutputParser:
chain = model | StrOutputParser()
response = chain.invoke("Hello")
# Resultado: "Hello there!"
```

### Padr√£o de mapeamento com dicion√°rio:
```python
# Mapeia sa√≠da de uma chain para entrada de outra
pipeline = {"variavel": chain_anterior} | proxima_chain

# Exemplo:
chain1 = template1 | model1 | StrOutputParser()
chain2 = template2 | model2 | StrOutputParser()
pipeline = {"texto_processado": chain1} | chain2
```

---

## üìù Script 5: 2-chains-e-processamento/5-sumarizacao.py

### Explica√ß√£o do RecursiveCharacterTextSplitter:
```python
splitter = RecursiveCharacterTextSplitter(
    chunk_size=250, chunk_overlap=70, 
)
parts = splitter.create_documents([long_text])
```

**O que √© RecursiveCharacterTextSplitter:**
- **Divide textos longos** em chunks menores
- **Mant√©m contexto** entre chunks com sobreposi√ß√£o
- **Permite processamento** de textos que excedem o limite dos modelos

**Par√¢metros importantes:**
- **`chunk_size=250`**: Tamanho m√°ximo de cada chunk (caracteres)
- **`chunk_overlap=70`**: Sobreposi√ß√£o entre chunks para manter contexto
- **`create_documents()`**: Converte texto em lista de documentos

### Explica√ß√£o da t√©cnica "stuff":
```python
chain_sumarize = load_summarize_chain(llm, chain_type="stuff", verbose=False)
```

**O que √© a t√©cnica "stuff":**
- **Coloca todo o texto** em um √∫nico prompt
- **Funciona bem** para textos que cabem no contexto do modelo
- **Processamento simples** e direto

**Vantagens:**
- ‚úÖ Simples de implementar
- ‚úÖ Mant√©m contexto completo
- ‚úÖ R√°pido para textos menores

**Limita√ß√µes:**
- ‚ùå Limitado pelo tamanho do contexto do modelo
- ‚ùå Pode ser ineficiente para textos muito longos

### Fluxo de processamento:
```python
# 1. Texto longo ‚Üí RecursiveCharacterTextSplitter
# 2. Chunks menores ‚Üí load_summarize_chain
# 3. T√©cnica "stuff" ‚Üí Resumo final
```

### Exemplo de uso:
```python
# Dividir texto
splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=70)
parts = splitter.create_documents([texto_longo])

# Sumarizar
chain = load_summarize_chain(llm, chain_type="stuff")
result = chain.invoke({"input_documents": parts})
resumo = result["output_text"]
```

### Quando usar a t√©cnica "stuff":
- ‚úÖ Textos que cabem no contexto do modelo
- ‚úÖ Sumariza√ß√µes simples e diretas
- ‚úÖ Quando voc√™ quer manter todo o contexto
- ‚úÖ Processamento r√°pido

---

## üîÑ Script 6: 2-chains-e-processamento/6-sumarizacao-com-map-reduce.py

### Explica√ß√£o da t√©cnica "map_reduce":
```python
chain_sumarize = load_summarize_chain(llm, chain_type="map_reduce", verbose=False)
```

**O que √© a t√©cnica "map_reduce":**
- **Processa cada chunk separadamente** (fase "map")
- **Combina os resumos** em um resumo final (fase "reduce")
- **Funciona com textos muito longos** que n√£o cabem no contexto

### Diferen√ßa detalhada entre "stuff" e "map_reduce":

**T√©cnica "stuff" (Script 5):**
```python
chain_type="stuff"
```
- **Processamento**: Coloca TODO o texto em um √∫nico prompt
- **Limita√ß√£o**: Limitado pelo tamanho do contexto do modelo
- **Velocidade**: Mais r√°pido (uma √∫nica chamada)
- **Contexto**: Mant√©m contexto completo
- **Uso**: Textos que cabem no contexto

**T√©cnica "map_reduce" (Script 6):**
```python
chain_type="map_reduce"
```
- **Processamento**: 
  1. **Map**: Sumariza cada chunk separadamente
  2. **Reduce**: Combina todos os resumos em um resumo final
- **Limita√ß√£o**: N√£o tem limite de tamanho (processa por partes)
- **Velocidade**: Mais lento (m√∫ltiplas chamadas)
- **Contexto**: Pode perder contexto entre chunks
- **Uso**: Textos muito longos

### Fluxo de processamento map_reduce:
```python
# Fase 1 - MAP: Processa cada chunk
Chunk 1 ‚Üí Sumariza√ß√£o ‚Üí Resumo 1
Chunk 2 ‚Üí Sumariza√ß√£o ‚Üí Resumo 2
Chunk 3 ‚Üí Sumariza√ß√£o ‚Üí Resumo 3
...

# Fase 2 - REDUCE: Combina os resumos
[Resumo 1, Resumo 2, Resumo 3, ...] ‚Üí Sumariza√ß√£o Final ‚Üí Resumo Final
```

### Compara√ß√£o pr√°tica:

**"Stuff" (Script 5):**
```python
# Uma √∫nica chamada
Texto completo ‚Üí Modelo ‚Üí Resumo
```

**"Map_reduce" (Script 6):**
```python
# M√∫ltiplas chamadas
Chunk 1 ‚Üí Modelo ‚Üí Resumo 1
Chunk 2 ‚Üí Modelo ‚Üí Resumo 2
...
[Resumos] ‚Üí Modelo ‚Üí Resumo Final
```

### Quando usar cada t√©cnica:

**Use "stuff" quando:**
- ‚úÖ Texto cabe no contexto do modelo
- ‚úÖ Quer processamento r√°pido
- ‚úÖ Precisa manter contexto completo
- ‚úÖ Textos at√© ~4000 tokens

**Use "map_reduce" quando:**
- ‚úÖ Texto √© muito longo (excede contexto)
- ‚úÖ Pode aceitar perda de contexto entre partes
- ‚úÖ Precisa processar documentos grandes
- ‚úÖ Textos com mais de ~4000 tokens

### Vantagens e desvantagens:

**"Stuff":**
- ‚úÖ R√°pido, mant√©m contexto, simples
- ‚ùå Limitado pelo contexto do modelo

**"Map_reduce":**
- ‚úÖ Funciona com textos ilimitados, escal√°vel
- ‚ùå Mais lento, pode perder contexto, complexo

---

## üîß Script 7: 2-chains-e-processamento/7-pipeline-de-sumarizacao.py

### Explica√ß√£o do pipeline manual map-reduce:
```python
# Fase MAP
map_prompt = PromptTemplate.from_template("Write a concise summary of the following text:\n{context}")
map_chain = map_prompt | llm | StrOutputParser()
prepare_map_inputs = RunnableLambda(lambda docs: [{"context": d.page_content} for d in docs])
map_stage = prepare_map_inputs | map_chain.map()

# Fase REDUCE
reduce_prompt = PromptTemplate.from_template("Combine the following summaries into a single concise summary:\n{context}")
reduce_chain = reduce_prompt | llm | StrOutputParser()
prepare_reduce_input = RunnableLambda(lambda summaries: {"context": "\n".join(summaries)})

# Pipeline completo
pipeline = map_stage | prepare_reduce_input | reduce_chain
```

### Diferen√ßa entre pipeline manual e load_summarize_chain:

**load_summarize_chain (Script 6):**
```python
chain = load_summarize_chain(llm, chain_type="map_reduce")
```
- ‚úÖ Mais simples de implementar
- ‚ùå Menos controle sobre cada etapa
- ‚ùå Prompts fixos (n√£o personaliz√°veis)

**Pipeline manual (Script 7):**
```python
# Controle total sobre cada etapa
map_prompt = PromptTemplate.from_template("Seu prompt personalizado")
reduce_prompt = PromptTemplate.from_template("Seu prompt personalizado")
```
- ‚úÖ Controle total sobre prompts
- ‚úÖ Personaliza√ß√£o completa
- ‚úÖ Flexibilidade m√°xima
- ‚ùå Mais c√≥digo para implementar

### Explica√ß√£o do PromptTemplate.from_template():
```python
map_prompt = PromptTemplate.from_template("Write a concise summary of the following text:\n{context}")
```

**Vantagem:**
- **Cria√ß√£o r√°pida** de templates simples
- **N√£o precisa definir** input_variables explicitamente
- **Detecta automaticamente** as vari√°veis no template

### Explica√ß√£o do m√©todo .map():
```python
map_stage = prepare_map_inputs | map_chain.map()
```

**O que faz:**
- **Aplica a chain** a cada item da lista de inputs
- **Processamento paralelo** de m√∫ltiplos documentos
- **Retorna lista** de resultados

### Fluxo detalhado do pipeline:

**Fase MAP:**
```python
# 1. prepare_map_inputs: Converte documentos em lista de dicion√°rios
# 2. map_chain.map(): Aplica sumariza√ß√£o a cada chunk
# 3. Resultado: Lista de resumos individuais
```

**Fase REDUCE:**
```python
# 1. prepare_reduce_input: Junta todos os resumos em uma string
# 2. reduce_chain: Combina os resumos em um resumo final
# 3. Resultado: Resumo final consolidado
```

### Vantagens do pipeline manual:
- **Controle total**: Personaliza cada etapa
- **Prompts customizados**: Adapta para seu caso de uso
- **Flexibilidade**: Pode adicionar etapas intermedi√°rias
- **Debugging**: F√°cil de identificar problemas
- **Otimiza√ß√£o**: Pode ajustar cada componente

### Exemplo de personaliza√ß√£o:
```python
# Prompt personalizado para sumariza√ß√£o
map_prompt = PromptTemplate.from_template(
    "Summarize this text in exactly 3 bullet points:\n{context}"
)

# Prompt personalizado para combina√ß√£o
reduce_prompt = PromptTemplate.from_template(
    "Create a poetic summary of these summaries:\n{context}"
)
```

---

# ü§ñ PASTA 3: AGENTES E TOOLS

---

## ü§ñ Script 1: 3-agentes-e-tools/1-agente-react-e-tools.py

### Explica√ß√£o do Agente ReAct:
```python
from langchain.agents import create_react_agent, AgentExecutor
```

**O que √© um Agente ReAct:**
- **ReAct**: Reasoning and Acting (Racioc√≠nio e A√ß√£o)
- **Raciocina** sobre qual ferramenta usar
- **Executa a√ß√µes** para obter informa√ß√µes
- **Combina** pensamento e a√ß√£o

### Explica√ß√£o do decorador @tool:
```python
@tool("calculator", return_direct=True)
def calculator(expression: str) -> str:
    """Evaluate a simple mathematical expression and return the result as a string."""
    return str(result)
```

**O que √© o decorador @tool:**
- **Transforma fun√ß√µes Python** em ferramentas do agente
- **`return_direct=True`**: Retorna resultado diretamente
- **`return_direct=False`**: Permite processamento adicional pelo agente

### Explica√ß√£o do formato ReAct:
```python
Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [tools]
Action Input: the input to the action
Observation: the result of the action
```

**Fluxo de racioc√≠nio:**
1. **Question**: Pergunta original
2. **Thought**: Agente pensa sobre o que fazer
3. **Action**: Escolhe qual ferramenta usar
4. **Action Input**: Par√¢metros para a ferramenta
5. **Observation**: Resultado da ferramenta
6. **Repete** at√© ter a resposta final

### Explica√ß√£o do AgentExecutor:
```python
agent_executor = AgentExecutor.from_agent_and_tools(
    agent=agent_chain, 
    tools=tools, 
    verbose=True, 
    handle_parsing_errors="Invalid format...",
    max_iterations=3
)
```

**Par√¢metros importantes:**
- **`verbose=True`**: Mostra o processo de racioc√≠nio
- **`max_iterations=3`**: Limita tentativas (evita loops infinitos)
- **`handle_parsing_errors`**: Mensagem para erros de formato

### Vantagens dos Agentes ReAct:
- **Racioc√≠nio expl√≠cito**: Pode ver como o agente pensa
- **Flexibilidade**: Pode usar m√∫ltiplas ferramentas
- **Extensibilidade**: F√°cil adicionar novas ferramentas
- **Controle**: Define limites e regras

### Exemplo de uso:
```python
# Criar ferramenta
@tool("search")
def search(query: str) -> str:
    return "Resultado da busca"

# Criar agente
tools = [search]
agent = create_react_agent(llm, tools, prompt)
executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools)

# Usar agente
result = executor.invoke({"input": "Pergunta do usu√°rio"})
```

### Diferen√ßa entre Tools e Fun√ß√µes normais:
- **Fun√ß√£o normal**: Executa diretamente
- **Tool**: Integrada ao agente, pode ser escolhida dinamicamente
- **Tool**: Tem descri√ß√£o que o agente usa para decidir

---

## üîó Script 2: 3-agentes-e-tools/2-agente-react-usando-prompt-hub.py

### Explica√ß√£o do Prompt Hub:
```python
from langchain import hub
prompt = hub.pull("hwchase17/react")
```

**O que √© o Prompt Hub:**
- **Reposit√≥rio de prompts** da comunidade LangChain
- **Prompts testados** e otimizados
- **Reutiliza√ß√£o** de templates comprovados
- **Padr√µes** estabelecidos pela comunidade

### Diferen√ßa entre prompt personalizado e Prompt Hub:

**Prompt personalizado (Script 1):**
```python
prompt = PromptTemplate.from_template("""
Answer the following questions as best you can...
{tools}
Use the following format:
Question: {input}
Thought: {agent_scratchpad}
""")
```
- ‚úÖ Controle total sobre o prompt
- ‚ùå Precisa escrever do zero
- ‚ùå Pode ter bugs ou inefici√™ncias

**Prompt Hub (Script 2):**
```python
prompt = hub.pull("hwchase17/react")
```
- ‚úÖ Prompt testado e otimizado
- ‚úÖ Criado por especialistas
- ‚úÖ Padr√£o da comunidade
- ‚ùå Menos flexibilidade

### Explica√ß√£o do "hwchase17/react":
```python
hub.pull("hwchase17/react")
```

**O que √©:**
- **"hwchase17"**: Harrison Chase (criador do LangChain)
- **"react"**: Prompt para agentes ReAct
- **Prompt oficial** do LangChain para ReAct
- **Testado extensivamente** pela comunidade

### Vantagens do Prompt Hub:
- **Qualidade**: Prompts testados e otimizados
- **Padroniza√ß√£o**: Usa formatos estabelecidos
- **Rapidez**: N√£o precisa criar prompts do zero
- **Confiabilidade**: Menos bugs e problemas
- **Comunidade**: Beneficia-se de melhorias coletivas

### Exemplo de outros prompts do hub:
```python
# Diferentes tipos de prompts dispon√≠veis
prompt1 = hub.pull("hwchase17/react")           # Agente ReAct
prompt2 = hub.pull("hwchase17/react-chat")      # ReAct para chat
prompt3 = hub.pull("hwchase17/zero-shot-react") # ReAct zero-shot
```

### Quando usar cada abordagem:

**Use Prompt Hub quando:**
- ‚úÖ Quer um prompt testado e confi√°vel
- ‚úÖ Est√° come√ßando com agentes
- ‚úÖ Precisa de padr√µes estabelecidos
- ‚úÖ Quer rapidez na implementa√ß√£o

**Use prompt personalizado quando:**
- ‚úÖ Precisa de controle total
- ‚úÖ Tem requisitos espec√≠ficos
- ‚úÖ Quer otimizar para seu caso de uso
- ‚úÖ Precisa de funcionalidades customizadas

### Fluxo de uso do Prompt Hub:
```python
# 1. Importar hub
from langchain import hub

# 2. Baixar prompt
prompt = hub.pull("hwchase17/react")

# 3. Usar no agente
agent = create_react_agent(llm, tools, prompt)
```

---

# üß† PASTA 4: GERENCIAMENTO DE MEM√ìRIA

---

## üß† Script 1: 4-gerenciamento-de-memoria/1-armazenamento-de-historico.py

### Explica√ß√£o do MessagesPlaceholder:
```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}"),
])
```

**O que √© MessagesPlaceholder:**
- **Reserva espa√ßo** no prompt para inserir mensagens do hist√≥rico
- **Vari√°vel din√¢mica** que √© preenchida com mensagens anteriores
- **Mant√©m contexto** entre m√∫ltiplas intera√ß√µes

### Explica√ß√£o do InMemoryChatMessageHistory:
```python
from langchain_core.chat_history import InMemoryChatMessageHistory

session_store: dict[str, InMemoryChatMessageHistory] = {}
```

**O que √© InMemoryChatMessageHistory:**
- **Armazena hist√≥rico** de mensagens em mem√≥ria
- **Tempor√°rio**: Perdido quando aplica√ß√£o √© fechada
- **R√°pido**: Acesso direto na mem√≥ria RAM
- **Simples**: Ideal para testes e desenvolvimento

### Explica√ß√£o do RunnableWithMessageHistory:
```python
from langchain_core.runnables import RunnableWithMessageHistory

conversational_chain = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history",
)
```

**Par√¢metros importantes:**
- **`chain`**: Chain base (prompt + modelo)
- **`get_session_history`**: Fun√ß√£o para obter hist√≥rico da sess√£o
- **`input_messages_key="input"`**: Chave para mensagem atual
- **`history_messages_key="history"`**: Chave para hist√≥rico no prompt

### Explica√ß√£o do gerenciamento de sess√µes:
```python
def get_session_history(session_id: str) -> InMemoryChatMessageHistory:
    if session_id not in session_store:
        session_store[session_id] = InMemoryChatMessageHistory()
    return session_store[session_id]

config = {"configurable": {"session_id": "demo-session"}}
```

**Como funciona:**
- **session_id**: Identifica unicamente uma conversa
- **session_store**: Dicion√°rio que armazena hist√≥ricos por sess√£o
- **get_session_history**: Cria ou recupera hist√≥rico da sess√£o
- **config**: Configura√ß√£o passada para identificar a sess√£o

### Fluxo de funcionamento:
```python
# 1. Primeira mensagem
response1 = conversational_chain.invoke({"input": "Hello, my name is Wesley"}, config=config)
# Hist√≥rico: [HumanMessage("Hello, my name is Wesley"), AIMessage("Hi Wesley!")]

# 2. Segunda mensagem
response2 = conversational_chain.invoke({"input": "What's my name?"}, config=config)
# Hist√≥rico: [mensagens anteriores + HumanMessage("What's my name?"), AIMessage("Your name is Wesley")]
```

### Vantagens da mem√≥ria de conversa:
- **Contexto cont√≠nuo**: Modelo lembra informa√ß√µes anteriores
- **Conversas naturais**: Pode referenciar mensagens passadas
- **M√∫ltiplas sess√µes**: Diferentes usu√°rios/conversas isoladas
- **Flexibilidade**: Pode ser implementada de v√°rias formas

### Limita√ß√µes do InMemoryChatMessageHistory:
- **Tempor√°rio**: Perdido ao reiniciar aplica√ß√£o
- **Limitado**: Pode consumir muita mem√≥ria com conversas longas
- **Sem persist√™ncia**: N√£o salva em banco de dados

### Exemplo de uso pr√°tico:
```python
# Configurar mem√≥ria
session_store = {}
def get_session_history(session_id: str):
    if session_id not in session_store:
        session_store[session_id] = InMemoryChatMessageHistory()
    return session_store[session_id]

# Criar chain com mem√≥ria
chain_with_memory = RunnableWithMessageHistory(
    chain, get_session_history,
    input_messages_key="input",
    history_messages_key="history"
)

# Usar com diferentes sess√µes
config1 = {"configurable": {"session_id": "user1"}}
config2 = {"configurable": {"session_id": "user2"}}
```

---

## üß† Script 2: 4-gerenciamento-de-memoria/2-historico-baseado-em-sliding-window.py

### Explica√ß√£o do trim_messages:
```python
from langchain_core.messages import trim_messages

trimmed = trim_messages(
    raw_history,
    token_counter=len,
    max_tokens=2,
    strategy="last",
    start_on="human",
    include_system=True,
    allow_partial=False,
)
```

**O que √© trim_messages:**
- **Controla tamanho** do hist√≥rico de mensagens
- **Reduz automaticamente** para caber no limite de tokens
- **Implementa sliding window** para manter contexto recente
- **Evita overflow** de contexto do modelo

### Par√¢metros importantes:

**`token_counter=len`:**
- **Fun√ß√£o para contar** tokens/mensagens
- **`len`**: Conta caracteres (simples para demonstra√ß√£o)
- **Em produ√ß√£o**: Use tokenizer real do modelo

**`max_tokens=2`:**
- **Limite m√°ximo** de tokens/mensagens
- **Muito baixo** neste exemplo para demonstrar o efeito
- **Em produ√ß√£o**: Use limite real do modelo (ex: 4000)

**`strategy="last"`:**
- **Estrat√©gia de corte**: manter as √∫ltimas mensagens
- **Alternativas**: `"first"` (primeiras), `"middle"` (meio)
- **Mais comum**: `"last"` para manter contexto recente

**`start_on="human"`:**
- **Ponto de in√≠cio** para contar tokens
- **`"human"`**: Come√ßa a contar de mensagens humanas
- **Alternativas**: `"assistant"`, `"system"`

**`include_system=True`:**
- **Inclui mensagem** do sistema no hist√≥rico
- **Importante**: Manter instru√ß√µes do sistema
- **Sempre True**: Para manter comportamento do assistente

**`allow_partial=False`:**
- **N√£o corta** mensagens no meio
- **Mant√©m integridade** das mensagens
- **Mais seguro**: Evita contexto incompleto

### Explica√ß√£o do Sliding Window:
```python
def prepare_inputs(payload: dict) -> dict:
    raw_history = payload.get("raw_history", [])
    trimmed = trim_messages(raw_history, ...)
    return {"input": payload.get("input",""), "history": trimmed}
```

**Como funciona:**
- **Hist√≥rico bruto**: Todas as mensagens da conversa
- **Processamento**: `trim_messages` reduz para limite
- **Sliding window**: Mant√©m apenas mensagens recentes
- **Resultado**: Modelo "esquece" informa√ß√µes antigas

### Diferen√ßa entre raw_history e history:
```python
conversational_chain = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="raw_history"  # Hist√≥rico bruto
)
```

**raw_history vs history:**
- **`raw_history`**: Hist√≥rico completo (antes do trim)
- **`history`**: Hist√≥rico processado (depois do trim)
- **Fluxo**: raw_history ‚Üí trim_messages ‚Üí history ‚Üí modelo

### Fluxo completo do Sliding Window:

**1. Armazenamento:**
```python
# Todas as mensagens s√£o salvas em raw_history
session_store[session_id] = InMemoryChatMessageHistory()
```

**2. Processamento:**
```python
# trim_messages reduz o hist√≥rico para caber no limite
trimmed = trim_messages(raw_history, max_tokens=2, strategy="last")
```

**3. Envio ao modelo:**
```python
# Modelo recebe apenas as mensagens mais recentes
chain.invoke({"input": "nova mensagem", "history": trimmed})
```

### Vantagens do Sliding Window:
- **Controle de custos**: Limita tokens enviados ao modelo
- **Performance**: Evita contextos muito longos
- **Conformidade**: Respeita limites de API
- **Flexibilidade**: Diferentes estrat√©gias de corte

### Limita√ß√µes do Sliding Window:
- **Perda de contexto**: Informa√ß√µes antigas s√£o perdidas
- **Configura√ß√£o complexa**: Precisa ajustar par√¢metros
- **Token counting**: Precisa de tokenizer preciso
- **Estrat√©gia de corte**: Pode perder informa√ß√µes importantes

### Exemplo pr√°tico de uso:
```python
# Configura√ß√£o realista
trimmed = trim_messages(
    raw_history,
    token_counter=tokenizer.count_tokens,  # Tokenizer real
    max_tokens=4000,  # Limite do modelo
    strategy="last",
    start_on="human",
    include_system=True,
    allow_partial=False,
)
```

### Compara√ß√£o com mem√≥ria simples:

**Mem√≥ria simples (Script 1):**
```python
# Mant√©m todo o hist√≥rico
history_messages_key="history"
```
- ‚úÖ Contexto completo
- ‚ùå Pode exceder limites
- ‚ùå Custo crescente

**Sliding Window (Script 2):**
```python
# Controla tamanho do hist√≥rico
history_messages_key="raw_history"
prepare_inputs = RunnableLambda(lambda p: trim_messages(p["raw_history"]))
```
- ‚úÖ Controle de custos
- ‚úÖ Performance consistente
- ‚ùå Perda de contexto antigo

---

## üìù Notas Gerais

### Diferen√ßa entre os m√©todos:
- **ChatOpenAI**: Espec√≠fico para OpenAI, mais direto
- **init_chat_model**: Gen√©rico, funciona com m√∫ltiplos provedores

### Dicas importantes:
- Sempre use `load_dotenv()` para carregar vari√°veis de ambiente
- Configure suas API keys no arquivo `.env`
- O m√©todo `invoke()` √© usado para fazer chamadas aos modelos
- Acesse a resposta com `.content`

---

*Este arquivo ser√° atualizado conforme avan√ßarmos no estudo dos scripts.*
